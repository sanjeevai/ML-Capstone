{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "\n",
    "## Capstone Project\n",
    "\n",
    "Sanjeev Yadav\n",
    "\n",
    "17 March 2018\n",
    "\n",
    "## I. Definition\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "Supervised learning is one of the most popular areas of machine learning in which much\n",
    "development has already taken place. In this project we are trying to identify the university-level\n",
    "factors which predict the presence of a strong retention and graduation rate. As the leader of the\n",
    "big data revolution, Google gathers information through clicks on the Internet and uses this\n",
    "information to personalize advertising to individual users<sup>[1]</sup>.\n",
    "\n",
    "The link to the data source is [here](https://github.com/sanjeevai/ML_Capstone). The name of the file is data.csv.\n",
    "Data was collected from [data.gov](https://catalog.data.gov/dataset/college-scorecard), but for ease of access we have downloaded it and pushed it to this GitHub repo.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "One of the most pressing issues facing American universities is the number of students who fail to graduate. Nearly one out of five four-year institutions graduate fewer than one-third of its first-time, full-time degree-seeking first-year students within six years. Although there are various explanations for attrition, we will try to identify the most important feature which affects the retention and graduation rates in 4-year institutions.\n",
    "\n",
    "We have two target variables:\n",
    "\n",
    "1. Graduation rate, and\n",
    "2. Retention rate\n",
    "\n",
    "Both are continuous variable so this is a regression task. We will train same regression models for both target variables but the final model will be chosen based on the `r2_score`. It may be the case that one model works good for graduation rate and some other works good for retention rate.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "We will use `r2_score` as the metric for performance of our model. In statistics, the coefficient of determination, denoted R2 or r2 and pronounced \"R squared\", is the proportion of the variance in the dependent variable that is predictable from the independent variable(s)<sup>[2]</sup>. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model<sup>[3][4][5]</sup>.\n",
    "\n",
    "$$r2 = 1 - RSS/TSS$$\n",
    "\n",
    "here RSS = sum of squares of difference between actual values(yi) and predicted values(yi^) and TSS = sum of squares of difference between actual values (yi) and mean value (Before applying Regression). So you can imagine TSS representing the best(actual) model, and RSS being in between our best model and the worst absolute mean model in which case we'll get RSS/TSS < 1. If our model is even worse than the worst mean model then in that case RSS > TSS(Since difference between actual observation and mean value < difference predicted value and actual observation)<sup>[6]</sup>.\n",
    "\n",
    "`r2_score` is a good metric for this problem because this is a regression problem and `r2_score` can provide a clear understanding of a regression model's performance by comparing the predicted value with true value in the simplest way.\n",
    "\n",
    "In our problem we have 2 target variables, both continuous and scaled using `StandardScaler` function from sklearn. So, `r2_score` is a fit metric for this problem.\n",
    "\n",
    "## II. Analysis\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "Name of the input data file is data.csv. It has 7593 observations and 123 variables.\n",
    "\n",
    "Information about all the variables can be seen in metadata.xlsx file. Let us discuss the variable in our input data.\n",
    "\n",
    "Features:\n",
    "\n",
    "**UNITID**: Unit ID for institution\n",
    "\n",
    "**OPEID**: 8-digit OPEID ID for institution\n",
    "\n",
    "**OPEID6**: 6-digit OPEID for institution\n",
    "\n",
    "**INSTNM**: Institution name\n",
    "\n",
    "**CITY**: city\n",
    "\n",
    "**STABBR**: State postcode\n",
    "\n",
    "**INSTURL**: URL for instution's homepage\n",
    "\n",
    "**NPCURL**: URL for institution's net price calculator\n",
    "\n",
    "**HCM2**: Schools that are on Heightened Cash Monitoring 2 by the Department of Education\n",
    "\n",
    "**PREDDEG**: Predominant undergraduate degree awarded. Can take 5 values:\n",
    "\n",
    "1. Not classified\n",
    "2. Predominantly certificate-degree granting\n",
    "3. Predominantly associate's-degree granting\n",
    "4. Predominantly bachelor's-degree granting\n",
    "5. Entirely graduate-degree granting\n",
    "\n",
    "**HIGHDEG**: Highest degree awarded. Can take 5 values:\n",
    "\n",
    "1. Non-degree-granting\n",
    "2. Certificate degree\n",
    "3. Associate degree\n",
    "4. Bachelor's degree\n",
    "5. Graduate degree\n",
    "\n",
    "**CONTROL**: Control of institution. Can take 3 values:\n",
    "\n",
    "1. Public\n",
    "2. Private non-profit\n",
    "3. Private for-profit\n",
    "\n",
    "**LOCALE**: Locale of institution. Can take 12 values:\n",
    "\n",
    "1. City: Large (population of 250,000 or more)\n",
    "2. City: Midsize (population of at least 100,000 but less than 250,000)\n",
    "3. City: Small (population less than 100,000)\n",
    "4. Suburb: Large (outside principal city, in urbanized area with population of 250,000 or more)\n",
    "5. Suburb: Midsize (outside principal city, in urbanized area with population of at least 100,000 but less than 250,000)\n",
    "6. Suburb: Small (outside principal city, in urbanized area with population less than 100,000)\n",
    "7. Town: Fringe (in urban cluster up to 10 miles from an urbanized area)\n",
    "8. Town: Distant (in urban cluster more than 10 miles and up to 35 miles from an urbanized area)\n",
    "9. Town: Remote (in urban cluster more than 35 miles from an urbanized area)\n",
    "10. Rural: Fringe (rural territory up to 5 miles from an urbanized area or up to 2.5 miles from an urban cluster)\n",
    "11. Rural: Distant (rural territory more than 5 miles but up to 25 miles from an urbanized area or more than 2.5 and up to 10 miles from an urban cluster)\n",
    "12. Rural: Remote (rural territory more than 25 miles from an urbanized area and more than 10 miles from an urban cluster)\n",
    "\n",
    "**HBCU**: Flag for historically Black College and University.\n",
    "\n",
    "**PBI**: Flag for predominantly black institution.\n",
    "\n",
    "**ANNHI**: Flag for Alaska Native Native Hawaiian serving institution.\n",
    "\n",
    "**TRIBAL**: Flag for tribal college and university\n",
    "\n",
    "**AANAPII**: Flag for Asian American Native American Pacific Islander-serving institution\n",
    "\n",
    "**HSI**: Flag for Hispanic-serving institution\n",
    "\n",
    "**NANTI**: Flag for Native American non-tribal institution\n",
    "\n",
    "**MENONLY**: Flag for men-only college\n",
    "\n",
    "**WOMENONLY**: Flag for women-only college\n",
    "\n",
    "**RELAFFIL**: Religious affiliation of the institution. It can take 65 values:\n",
    "\n",
    "1. Not reported\n",
    "2. Not applicable\n",
    "3. American Evangelical Lutheran Church\n",
    "4. African Methodist Episcopal Zion Church\n",
    "5. Assemblies of God Church\n",
    "6. Brethren Church\n",
    "7. Roman Catholic\n",
    "8. Wisconsin Evangelical Lutheran Synod\n",
    "9. Christ and Missionary Alliance Church\n",
    "10. Christian Reformed Church\n",
    "11. Evangelical Congregational Church\n",
    "12. Evangelical Covenant Church of America\n",
    "13. Evangelical Free Church of America\n",
    "14. Evangelical Lutheran Church\n",
    "15. International United Pentecostal Church\n",
    "16. Free Will Baptist Church\n",
    "17. Interdenominational\n",
    "18. Mennonite Brethren Church\n",
    "19. Moravian Church\n",
    "20. North American Baptist\n",
    "21. Pentecostal Holiness Church\n",
    "22. Christian Churches and Churches of Christ\n",
    "23. Reformed Church in America\n",
    "24. Episcopal Church, Reformed\n",
    "25. African Methodist Episcopal\n",
    "26. American Baptist\n",
    "27. American Lutheran\n",
    "28. Baptist\n",
    "29. Christian Methodist Episcopal\n",
    "30. Church of God\n",
    "31. Church of Brethren\n",
    "32. Church of the Nazarene\n",
    "33. Cumberland Presbyterian\n",
    "34. Christian Church (Disciples of Christ)\n",
    "35. Free Methodist\n",
    "36. Friends\n",
    "37. Presbyterian Church (USA)\n",
    "38. Lutheran Church in America\n",
    "39. Lutheran Church - Missouri Synod\n",
    "40. Mennonite Church\n",
    "41. United Methodist\n",
    "42. Protestant Episcopal\n",
    "43. Churches of Christ\n",
    "44. Southern Baptist\n",
    "45. United Church of Christ\n",
    "46. Protestant, not specified\n",
    "47. Multiple Protestant Denomination\n",
    "48. Other Protestant\n",
    "49. Jewish\n",
    "50. Reformed Presbyterian Church\n",
    "51. United Brethren Church\n",
    "52. Missionary Church Inc\n",
    "53. Undenominational\n",
    "54. Wesleyan\n",
    "55. Greek Orthodox\n",
    "56. Russian Orthodox\n",
    "57. Unitarian Universalist\n",
    "58. Latter Day Saints (Mormon Church)\n",
    "59. Seventh Day Adventists\n",
    "60. The Presbyterian Church in America\n",
    "61. Other (none of the above)\n",
    "62. Original Free Will Baptist\n",
    "63. Ecumenical Christian\n",
    "64. Evangelical Christian\n",
    "65. Presbyterian\n",
    "\n",
    "**SATVR25**: 25th percentile of SAT scores at the institution (critical reading)\n",
    "\n",
    "**SATVR75**: 75th percentile of SAT scores at the institution (critical reading)\n",
    "\n",
    "**SATMT25**: 25th percentile of SAT scores at the institution (math)\n",
    "\n",
    "**SATMT75**: 75th percentile of SAT scores at the institution (math)\n",
    "\n",
    "**SATWR25**: 25th percentile of SAT scores at the institution (writing)\n",
    "\n",
    "**SATWR75**: 75th percentile of SAT scores at the institution (writing)\n",
    "\n",
    "**SATVRMID**: Midpoint of SAT scores at the institution (critical reading)\n",
    "\n",
    "**SATMTMID**: Midpoint of SAT scores at the institution (math)\n",
    "\n",
    "**SATWRMID**: Midpoint of SAT scores at the institution (writing)\n",
    "\n",
    "**ACTCM25**: 25th percentile of the ACT cumulative score\n",
    "\n",
    "**ACTCM75**: 75th percentile of the ACT cumulative score\n",
    "\n",
    "**ACTEN25**: 25th percentile of the ACT English score\n",
    "\n",
    "**ACTEN75**: 75th percentile of the ACT English score\n",
    "\n",
    "**ACTMT25**: 25th percentile of the ACT math score\n",
    "\n",
    "**ACTMT75**: 75th percentile of the ACT math score\n",
    "\n",
    "**ACTWR25**: 25th percentile of the ACT writing score\n",
    "\n",
    "**ACTWR75**: 75th percentile of the ACT writing score\n",
    "\n",
    "**ACTCMMID**: Midpoint of the ACT cumulative score\n",
    "\n",
    "**ACTENMID**: Midpoint of the ACT English score\n",
    "\n",
    "**ACTMTMID**: Midpoint of the ACT math score\n",
    "\n",
    "**ACTWRMID**: Midpoint of the ACT writing score\n",
    "\n",
    "**SAT_AVG**: Average SAT equivalent score of students admitted\n",
    "\n",
    "**SAT_AVG_ALL**:Average SAT equivalent score of students admitted for all campuses rolled up to the 6-digit OPE ID\n",
    "\n",
    "**PCIP01**: Percentage of degrees awarded in Agriculture, Agriculture Operations, And Related Sciences.\n",
    "\n",
    "**PCIP03**: Percentage of degrees awarded in Natural Resources And Conservation.\n",
    "\n",
    "**PCIP04**: Percentage of degrees awarded in Architecture And Related Services.\n",
    "\n",
    "**PCIP05**: Percentage of degrees awarded in Area, Ethnic, Cultural, Gender, And Group Studies.\n",
    "\n",
    "**PCIP09**: Percentage of degrees awarded in Communication, Journalism, And Related Programs.\n",
    "\n",
    "**PCIP10**: Percentage of degrees awarded in Communications Technologies/Technicians And Support Services.\n",
    "\n",
    "**PCIP11**: Percentage of degrees awarded in Computer And Information Sciences And Support Services.\n",
    "\n",
    "**PCIP12**: Percentage of degrees awarded in Personal And Culinary Services.\n",
    "\n",
    "**PCIP13**: Percentage of degrees awarded in Education.\n",
    "\n",
    "**PCIP14**: Percentage of degrees awarded in Engineering.\n",
    "\n",
    "**PCIP15**: Percentage of degrees awarded in Engineering Technologies And Engineering-Related Fields.\n",
    "\n",
    "**PCIP16**: Percentage of degrees awarded in Foreign Languages, Literatures, And Linguistics.\n",
    "\n",
    "**PCIP19**: Percentage of degrees awarded in Family And Consumer Sciences/Human Sciences.\n",
    "\n",
    "**PCIP22**: Percentage of degrees awarded in Legal Professions And Studies.\n",
    "\n",
    "**PCIP23**: Percentage of degrees awarded in English Language And Literature/Letters.\n",
    "\n",
    "**PCIP24**: Percentage of degrees awarded in Liberal Arts And Sciences, General Studies And Humanities.\n",
    "\n",
    "**PCIP25**: Percentage of degrees awarded in Library Science.\n",
    "\n",
    "**PCIP26**: Percentage of degrees awarded in Biological And Biomedical Sciences.\n",
    "\n",
    "**PCIP27**: Percentage of degrees awarded in Mathematics And Statistics.\n",
    "\n",
    "**PCIP29**: Percentage of degrees awarded in Military Technologies And Applied Sciences.\n",
    "\n",
    "**PCIP30**: Percentage of degrees awarded in Multi/Interdisciplinary Studies.\n",
    "\n",
    "**PCIP31**: Percentage of degrees awarded in Parks, Recreation, Leisure, And Fitness Studies.\n",
    "\n",
    "**PCIP38**: Percentage of degrees awarded in Philosophy And Religious Studies.\n",
    "\n",
    "**PCIP39**: Percentage of degrees awarded in Theology And Religious Vocations.\n",
    "\n",
    "**PCIP40**: Percentage of degrees awarded in Physical Sciences.\n",
    "\n",
    "**PCIP41**: Percentage of degrees awarded in Science Technologies/Technicians.\n",
    "\n",
    "**PCIP42**: Percentage of degrees awarded in Psychology.\n",
    "\n",
    "**PCIP43**: Percentage of degrees awarded in Homeland Security, Law Enforcement, Firefighting And Related Protective \n",
    "Services.\n",
    "\n",
    "**PCIP44**: Percentage of degrees awarded in Public Administration And Social Service Professions.\n",
    "\n",
    "**PCIP45**: Percentage of degrees awarded in Social Sciences.\n",
    "\n",
    "**PCIP46**: Percentage of degrees awarded in Construction Trades.\n",
    "\n",
    "**PCIP47**: Percentage of degrees awarded in Mechanic And Repair Technologies/Technicians.\n",
    "\n",
    "**PCIP48**: Percentage of degrees awarded in Precision Production.\n",
    "\n",
    "**PCIP49**: Percentage of degrees awarded in Transportation And Materials Moving.\n",
    "\n",
    "**PCIP50**: Percentage of degrees awarded in Visual And Performing Arts.\n",
    "\n",
    "**PCIP51**: Percentage of degrees awarded in Health Professions And Related Programs.\n",
    "\n",
    "**PCIP52**: Percentage of degrees awarded in Business, Management, Marketing, And Related Support Services.\n",
    "\n",
    "**PCIP54**: Percentage of degrees awarded in History.\n",
    "\n",
    "**DISTANCEONLY**: Flag for distance-education-only education\n",
    "\n",
    "**UGDS**: Enrollment of undergraduate certificate/degree-seeking students\n",
    "\n",
    "**UGDS_WHITE**: Total share of enrollment of undergraduate degree-seeking students who are white\n",
    "\n",
    "**UGDS_BLACK**: Total share of enrollment of undergraduate degree-seeking students who are black\n",
    "\n",
    "**UGDS_HISP**: Total share of enrollment of undergraduate degree-seeking students who are Hispanic\n",
    "\n",
    "**UGDS_ASIAN**: Total share of enrollment of undergraduate degree-seeking students who are Asian\n",
    "\n",
    "**UGDS_AIAN**: Total share of enrollment of undergraduate degree-seeking students who are American Indian/Alaska Native\n",
    "\n",
    "**UGDS_NHPI**: Total share of enrollment of undergraduate degree-seeking students who are Native Hawaiian/Pacific \n",
    "Islander\n",
    "\n",
    "**UGDS_2MOR**: Total share of enrollment of undergraduate degree-seeking students who are two or more races\n",
    "\n",
    "**UGDS_NRA**: Total share of enrollment of undergraduate degree-seeking students who are non-resident aliens\n",
    "\n",
    "**UGDS_UNKN**: Total share of enrollment of undergraduate degree-seeking students whose race is unknown\n",
    "\n",
    "**PPTUG_EF**: Share of undergraduate, degree-/certificate-seeking students who are part-time \n",
    "\n",
    "**CURROPER**: Flag for currently operating institution, 0=closed, 1=operating\n",
    "\n",
    "**NPT4_PUB**: Average net price for Title IV institutions (public institutions)\n",
    "\n",
    "**NPT4_PRIV**: Average net price for Title IV institutions (private for-profit and nonprofit institutions)\n",
    "\n",
    "**NPT41_PUB**: Average net price for \\$0-$30,000 family income (public institutions)\n",
    "\n",
    "**NPT42_PUB**: Average net price for \\$30,001-$48,000 family income (public institutions)\n",
    "\n",
    "**NPT43_PUB**: Average net price for \\$48,001-$75,000 family income (public institutions)\n",
    "\n",
    "**NPT44_PUB**: Average net price for \\$75,001-$110,000 family income (public institutions)\n",
    "\n",
    "**NPT45_PUB**: Average net price for \\$110,000+ family income (public institutions)\n",
    "\n",
    "**NPT41_PRIV**: Average net price for \\$0-$30,000 family income (private for-profit and nonprofit institutions)\n",
    "\n",
    "**NPT42_PRIV**: Average net price for \\$30,001-$48,000 family income (private for-profit and nonprofit institutions)\n",
    "\n",
    "**NPT43_PRIV**: Average net price for \\$48,001-$75,000 family income (private for-profit and nonprofit institutions)\n",
    "\n",
    "**NPT44_PRIV**: Average net price for \\$75,001-$110,000 family income (private for-profit and nonprofit institutions)\n",
    "\n",
    "**NPT45_PRIV**: Average net price for \\$110,000+ family income (private for-profit and nonprofit institutions)\n",
    "\n",
    "**PCTPELL**: Percentage of undergraduates who receive a Pell Grant\n",
    "\n",
    "**PCTFLOAN**: Percent of all undergraduate students receiving a federal student loan\n",
    "\n",
    "**UG25ABV**: Percentage of undergraduates aged 25 and above\n",
    "\n",
    "**MD_EARN_WNE_P10**: Median earnings of students working and not enrolled 10 years after entry\n",
    "\n",
    "**GT_25K_P6**: Share of students earning over $25,000/year (threshold earnings) 6 years after entry\n",
    "\n",
    "**GRAD_DEBT_MDN_SUPP**: Median debt of completers, suppressed for n=30\n",
    "\n",
    "**GRAD_DEBT_MDN10YR_SUPP**: Median debt of completers expressed in 10-year monthly payments, suppressed for n=30\n",
    "\n",
    "**RPY_3YR_RT_SUPP**: 3-year repayment rate, suppressed for n=30\n",
    "\n",
    "Target variables:\n",
    "\n",
    "1. For graduation rates we have two variables in our data. Let us see the difference between those two:\n",
    "\n",
    "1.1. **rate_suppressed.four_year**\n",
    "\n",
    "Completion rate for first-time, full-time students at four-year institutions (150% of expected time to completion) , pooled in two-year rolling averages and suppressed for small n size.\n",
    "\n",
    "1.2. **rate_suppressed.lt_four_year_150percent**\n",
    "\n",
    "Completion rate for first-time, full-time students at less-than-four-year institutions (150% of expected time to completion) , pooled in two-year rolling averages and suppressed for small n size\n",
    "\n",
    "We will be making predictions for 4-year institutions.\n",
    "\n",
    "2. For retention rates we have four variables in our data. Let us see the difference between them:\n",
    "\n",
    "2.1. **retention_rate.four_year.full_time**\n",
    "\n",
    "First-time, full-time student retention rate at four-year institutions.\n",
    "\n",
    "2.2. **retention_rate.lt_four_year.full_time**\n",
    "\n",
    "First-time, full-time student retention rate at less-than-four-year institutions.\n",
    "\n",
    "2.3. **retention_rate.four_year.part_time**\n",
    "\n",
    "First-time, part-time student retention rate at four-year institutions\n",
    "\n",
    "2.4. **retention_rate.lt_four_year.part_time**\n",
    "\n",
    "First-time, part-time student retention rate at four-year institutions\n",
    "\n",
    "Retention rate is for full-time students and we are making predictions for 4-year institutions. So, our target variable is **retention_rate.four_year.full_time**.\n",
    "\n",
    "So there are 2 response variables:\n",
    "\n",
    "1. **rate_suppressed.four_year**: This follows a normal distribution.\n",
    "2. **retention_rate.four_year.full_time**: This follow a left skewed distribution.\n",
    "\n",
    "### Algorithms and Techniques\n",
    "\n",
    "We have one benchmark model and 5 other supervised regression models. Below is the explanation of each model:\n",
    "\n",
    "1. Linear Regression(Benchmark Model)\n",
    "\n",
    "After data preprocessing, we train the input data and the evaluation metric from this model was considered as the benchmark.\n",
    "\n",
    "A simple linear regression uses only one independent variable, and it describes the relationship between the independent variable and dependent variable(s) as a straight line.\n",
    "\n",
    "Here we have 2 dependent variable so we train our model 2 times.\n",
    "\n",
    "2. AdaBoost Regressor\n",
    "\n",
    "AdaBoost, short for Adaptive Boosting, is a machine learning algorithm formulated by Yoav Freund and Robert Schapire, who won the 2003 Gödel Prize for their work. AdaBoost is sensitive to noisy data and outliers. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing, the final model can be proven to converge to a strong learner.\n",
    "\n",
    "3. Decision Tree Regressor\n",
    "\n",
    "Decision tree builds regression or classification models in the form of a tree structure. It brakes down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.\n",
    "\n",
    "4. Extra Trees Regressor\n",
    "\n",
    "With respect to random forests, the method drops the idea of using bootstrap copies of the learning sample, and instead of trying to find an optimal cut-point for each one of the K randomly chosen features at each node, it selects a cut-point at random.\n",
    "\n",
    "4. Gradient Boosting Regressor\n",
    "\n",
    "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.\n",
    "\n",
    "5. Light GBM\n",
    "\n",
    "Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.\n",
    "\n",
    "6. Random Forest Regressor\n",
    "\n",
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n",
    "\n",
    "### Benchmark\n",
    "\n",
    "We will consider benchmark model as the initial linear regressor. We will get the `r2_score` from this model. Then we will use other regression models to improve our score.\n",
    "\n",
    "## III. Methodology\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "We have many column which were unique for every observation. We will choose one of them as the identifier variable for each observation.\n",
    "\n",
    "For SAT and ACT scores we have 25 percentile, 75 percentile and mid-point values.We will only use the midpoint values for calculation to avoid curse of dimensionality.\n",
    "\n",
    "We have two columns for SAT Average: SAT_AVG and SAT_AVG_ALL. Since we have removed OPEID column, we will be using SAT_AVG column because it provides overall stats, rather than averages based on OPEID.\n",
    "\n",
    "We have average net price for public and private institutions. We will remove the average price based on different family income levels and use the overall average net price for public and private institutions. We am removing columns related to family income levels because We don't that family income is a university level factor. This is a factor based on student level.\n",
    "\n",
    "Now I am removing the **Categorical Columns ( dtype object )** which have many levels.\n",
    "\n",
    "I am keeping categorical columns which have less than 10 levels.\n",
    "\n",
    "`INSTNM`, `INSTURL` & `NPCURL` are identifiers for colleges in offline or online media and are not university level factors which affect education's status.\n",
    "\n",
    "`STABBR`, `CITY` & `RELAFFIL` columns have been removed because they had too many levels to be considered.\n",
    "\n",
    "Calling `.info()` method shows data type _\"object\"_ for last few columns. This is because some values in these columns are \"**PrivacySuppressed**\".\n",
    "\n",
    "There are two types of invalid entries here. First is the `Nan` entry and another is `PrivacySuppressed`.\n",
    "\n",
    "We will first convert `PrivacySuppressed` to null value and then replace all the null values accordingly.\n",
    "\n",
    "After converting `PrivacySuppressed` to `NaN`, pandas still treats them as _object_ data type. Below is a table showing correct data type of these columns(from metadata.xlsx file):\n",
    "\n",
    "| Column| Data type |\n",
    "|------|------|\n",
    "|MD_EARN_WNE_P10| integer|\n",
    "|GT_25K_P6|float|\n",
    "|GRAD_DEBT_MDN_SUPP|float|\n",
    "|GRAD_DEBT_MDN10YR_SUPP|float|\n",
    "|RPY_3YR_RT_SUPP|float|\n",
    "|C150_L4_POOLED_SUPP|float|\n",
    "|C150_4_POOLED_SUPP|float|\n",
    "\n",
    "Converting `MD_EARN_WNE_P10` to integer data type will throw an error because NaN cannot be converted to integer.\n",
    "\n",
    "So we will convert `MD_EARN_WNE_P10` to _float_ type.\n",
    "\n",
    "Now we will handle the null values.\n",
    "\n",
    "But before applying median imputation, we need to check for some other categorical columns which might be important for analysis. There are some categorical levels which cannot be ignored while building the model. Those columns are:\n",
    "\n",
    "1. PREDDEG\n",
    "2. HIGHDEG\n",
    "3. CONTROL\n",
    "4. LOCALE(contains 12 levels but I will reduce them to 4 levels)\n",
    "\n",
    "We have created dummy variable for these columns.\n",
    "\n",
    "Finally we renamed the variables to be user-friendly.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "We have the same training and testing features for all the models. First we have a linear regression model which is our benchmark model. The `r2_score` obtained from this model are:\n",
    "\n",
    "$$graduation\\ rate  = 0.44$$\n",
    "$$retention\\ rate  = 0.25$$\n",
    "\n",
    "Now we will use other regression models.\n",
    "\n",
    "1. AdaBoost Regressor\n",
    "\n",
    "`r2_score`s obtained from this model were:\n",
    "\n",
    "$$graduation\\ rate  = -0.04$$\n",
    "$$retention\\ rate  = -0.07$$\n",
    "\n",
    "2. Decision Tree Regressor\n",
    "\n",
    "`r2_score`s obtained from this model were:\n",
    "\n",
    "$$graduation\\ rate  = 0.38$$\n",
    "$$retention\\ rate  = 0.21$$\n",
    "\n",
    "3. Extra Trees Regressor\n",
    "\n",
    "`r2_score`s obtained from this model were:\n",
    "\n",
    "$$graduation\\ rate  = 0.31$$\n",
    "$$retention\\ rate  = 0.17$$\n",
    "\n",
    "4. Gradient Boosting Regressor\n",
    "\n",
    "`r2_score`s obtained from this model were:\n",
    "\n",
    "$$graduation\\ rate  = 0.49$$\n",
    "$$retention\\ rate  = 0.30$$\n",
    "\n",
    "After hyperparameter tuning the scores were\n",
    "\n",
    "$$graduation\\ rate  = 0.69$$\n",
    "$$retention\\ rate  = 0.34$$\n",
    "\n",
    "5. Light GBM\n",
    "\n",
    "`r2_score`s obtained from this model were:\n",
    "\n",
    "$$graduation\\ rate  = 0.51$$\n",
    "$$retention\\ rate  = 0.12$$\n",
    "\n",
    "After hyperparameter tuning the scores were\n",
    "\n",
    "$$graduation\\ rate  = 0.83$$\n",
    "$$retention\\ rate  = 0.58$$\n",
    "\n",
    "6. Random Forest Regressor\n",
    "\n",
    "`r2_score`s obtained from this model were:\n",
    "\n",
    "$$graduation\\ rate  = 0.33$$\n",
    "$$retention\\ rate  = 0.17$$\n",
    "\n",
    "\n",
    "\n",
    "### Refinement\n",
    "\n",
    "Our benchmark model was linear regressor. For this model, `r2_score` for graduation rate was 0.44 and for retention rate was 0.25.\n",
    "\n",
    "The final model which we chose was Light GBM. Initial r2 scores for this model were 0.51 for graduation rates and 0.12 for retention rates. We applied hyperparameter tuning by using `num_iteration=gbm.best_iteration_` for the model. Here `gbm` is our regressor model.\n",
    "\n",
    "Final metrics were 0.83 for graduation rates and 0.57 for retention rates.\n",
    "\n",
    "Here is the full process for hyperparameter tuning.\n",
    "\n",
    "For graduation rates:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lgb_grad_train = lgb.Dataset(X_train, grad_train)\n",
    "lgb_grad_eval = lgb.Dataset(X_test, grad_test, reference=lgb_grad_train)\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.LGBMRegressor(objective='regression',\n",
    "                        num_leaves=31,\n",
    "                        learning_rate=0.05,\n",
    "                        n_estimators=20)\n",
    "gbm.fit(X_train, grad_train,\n",
    "        eval_set=[(X_test, grad_test)],\n",
    "        eval_metric='l1',\n",
    "        early_stopping_rounds=5)\n",
    "\n",
    "print('Start predicting...')\n",
    "# predict\n",
    "grad_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
    "# eval\n",
    "print('The r2_score for graduation rate is:', r2_score(grad_test, grad_pred) ** 0.5)\n",
    "\n",
    "# feature importances\n",
    "print('Feature importances:', list(gbm.feature_importances_))\n",
    "\n",
    "# other scikit-learn modules\n",
    "estimator = lgb.LGBMRegressor(num_leaves=31)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [20, 40]\n",
    "}\n",
    "\n",
    "gbm = GridSearchCV(estimator, param_grid)\n",
    "\n",
    "gbm.fit(X_train, grad_train)\n",
    "\n",
    "print('Best parameters found by grid search are:', gbm.best_params_)\n",
    "```\n",
    "Best parameters found by grid search are: {'learning_rate': 0.1, 'n_estimators': 40}\n",
    "\n",
    "For retention rates:\n",
    "\n",
    "```python\n",
    "lgb_ret_train = lgb.Dataset(X_train, ret_train)\n",
    "lgb_ret_eval = lgb.Dataset(X_test, ret_test, reference=lgb_ret_train)\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.LGBMRegressor(objective='regression',\n",
    "                        num_leaves=31,\n",
    "                        learning_rate=0.05,\n",
    "                        n_estimators=20)\n",
    "gbm.fit(X_train, ret_train,\n",
    "        eval_set=[(X_test, ret_test)],\n",
    "        eval_metric='l1',\n",
    "        early_stopping_rounds=5)\n",
    "\n",
    "print('Start predicting...')\n",
    "# predict\n",
    "ret_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
    "# eval\n",
    "print('The r2_score for retention rate is:', r2_score(ret_test, ret_pred) ** 0.5)\n",
    "\n",
    "# feature importances\n",
    "print('Feature importances:', list(gbm.feature_importances_))\n",
    "\n",
    "# other scikit-learn modules\n",
    "estimator = lgb.LGBMRegressor(num_leaves=31)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [20, 40]\n",
    "}\n",
    "\n",
    "gbm = GridSearchCV(estimator, param_grid)\n",
    "\n",
    "gbm.fit(X_train, ret_train)\n",
    "\n",
    "print('Best parameters found by grid search are:', gbm.best_params_)\n",
    "```\n",
    "\n",
    "The best parameters obtained retention rates are: {'learning_rate': 0.1, 'n_estimators': 20}\n",
    "\n",
    "## IV. Results\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "\n",
    "The final model was Light GBM. It was chosen for graduation rates only. But, when we applied hyperparameter tuning then it was the best model for retention rates also.\n",
    "\n",
    "Below are paramters of the final model:\n",
    "\n",
    "1. `objective='regression'`\n",
    "\n",
    "This indicates that we ae doing a regression problem.\n",
    "\n",
    "2. num_leaves=31\n",
    "\n",
    "It shows number of leaves in one tree\n",
    "\n",
    "3. learning_rate=0.1\n",
    "\n",
    "It is the learning rate of model\n",
    "\n",
    "4. n_estimators=40\n",
    "\n",
    "It is the number of trees to be used in the forest.\n",
    "\n",
    "### Justification\n",
    "\n",
    "Below is a comparing of our final model of our final model and the benchmark model showing r2 score for prediction of graduation and retention rates.\n",
    "\n",
    "|Model|graduation rates|retention rates |\n",
    "|------|------|------|\n",
    "|Linear Regression(Benchmark model)|0.44|0.25|\n",
    "|Light GBM(Final model)|0.83|0.58|\n",
    "\n",
    "We do not expect a very high r2 score because our aim is not to get a very good prediction. Our aim is to see the important features. So, it will be same when we have best model, whatever high be the r2 score.\n",
    "\n",
    "## V. Conclusion\n",
    "\n",
    "### Reflection\n",
    "\n",
    "We had 7593 observations and 123 variables. The toughest part was feature selection. We have used domain knowledge to remove features which we think are important or are not university level factor.\n",
    "\n",
    "One interesting aspect was that we had multiple target labels and we have to narrow down by applying certain conditions, like doing the problem for first-time full-time 4-year institutions.\n",
    "\n",
    "My initial and final model can be used for this problem in general because they provide a good result of the important university level factors affecting education.\n",
    "\n",
    "### Improvement\n",
    "\n",
    "Further improvements can be made if we use hyperparameter tuning on the remaining models. Maybe they will give results than our final model.\n",
    "\n",
    "We have not used XGBoost algorithm here. Maybe it can be used in further iteration to improve the results. I did not use XGBoost because I did not know in detail how it works.\n",
    "\n",
    "If we consider our final solution as the new benchmark then I think better solutions exist because we have obtained r2 scores of 0.83 and 0.58 only. There are good chances of improvement.\n",
    "\n",
    "## VI References\n",
    "\n",
    "1. https://www.usnews.com/opinion/articles/2013/08/15/why-big-data-not-moocs-will-revolutionize-education\n",
    "\n",
    "2. http://stattrek.com/statistics/dictionary.aspx?definition=coefficient_of_determination\n",
    "3. Steel, R. G. D.; Torrie, J. H. (1960). Principles and Procedures of Statistics with Special Reference to the Biological Sciences. McGraw Hill.\n",
    "4. Glantz, Stanton A.; Slinker, B. K. (1990). Primer of Applied Regression and Analysis of Variance. McGraw-Hill. ISBN 0-07-023407-8.\n",
    "5. Draper, N. R.; Smith, H. (1998). Applied Regression Analysis. Wiley-Interscience. ISBN 0-471-17082-8.\n",
    "6. https://stackoverflow.com/questions/23309073/how-is-the-r2-value-in-scikit-learn-calculated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
