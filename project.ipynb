{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path\n",
    "sys.path.append('/anaconda3/lib/python3.6/site-packages/')\n",
    "sys.path.append('/usr/local/lib/python3.6/site-packages/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNITID</th>\n",
       "      <th>OPEID</th>\n",
       "      <th>OPEID6</th>\n",
       "      <th>INSTNM</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STABBR</th>\n",
       "      <th>INSTURL</th>\n",
       "      <th>NPCURL</th>\n",
       "      <th>HCM2</th>\n",
       "      <th>PREDDEG</th>\n",
       "      <th>...</th>\n",
       "      <th>RET_PTL4</th>\n",
       "      <th>PCTFLOAN</th>\n",
       "      <th>UG25ABV</th>\n",
       "      <th>MD_EARN_WNE_P10</th>\n",
       "      <th>GT_25K_P6</th>\n",
       "      <th>GRAD_DEBT_MDN_SUPP</th>\n",
       "      <th>GRAD_DEBT_MDN10YR_SUPP</th>\n",
       "      <th>RPY_3YR_RT_SUPP</th>\n",
       "      <th>C150_L4_POOLED_SUPP</th>\n",
       "      <th>C150_4_POOLED_SUPP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100654</td>\n",
       "      <td>100200</td>\n",
       "      <td>1002</td>\n",
       "      <td>Alabama A &amp; M University</td>\n",
       "      <td>Normal</td>\n",
       "      <td>AL</td>\n",
       "      <td>www.aamu.edu/</td>\n",
       "      <td>www2.aamu.edu/scripts/netpricecalc/npcalc.htm</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8159</td>\n",
       "      <td>0.0877</td>\n",
       "      <td>29900</td>\n",
       "      <td>0.453</td>\n",
       "      <td>35000</td>\n",
       "      <td>361.891446885773</td>\n",
       "      <td>0.2458495231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UNITID   OPEID  OPEID6                    INSTNM    CITY STABBR  \\\n",
       "0  100654  100200    1002  Alabama A & M University  Normal     AL   \n",
       "\n",
       "         INSTURL                                         NPCURL  HCM2  \\\n",
       "0  www.aamu.edu/  www2.aamu.edu/scripts/netpricecalc/npcalc.htm     0   \n",
       "\n",
       "   PREDDEG         ...          RET_PTL4  PCTFLOAN  UG25ABV  MD_EARN_WNE_P10  \\\n",
       "0        3         ...               NaN    0.8159   0.0877            29900   \n",
       "\n",
       "   GT_25K_P6  GRAD_DEBT_MDN_SUPP  GRAD_DEBT_MDN10YR_SUPP  RPY_3YR_RT_SUPP  \\\n",
       "0      0.453               35000        361.891446885773     0.2458495231   \n",
       "\n",
       "   C150_L4_POOLED_SUPP  C150_4_POOLED_SUPP  \n",
       "0                  NaN              0.3303  \n",
       "\n",
       "[1 rows x 123 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# load the dataset\n",
    "df  = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# display the first record\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7593, 123)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Data Exploration\n",
    "\n",
    "Total number of records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7593"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featureset exploration:\n",
    "\n",
    "**UNITID**: Unit ID for institution\n",
    "\n",
    "**OPEID**: 8-digit OPEID ID for institution\n",
    "\n",
    "**OPEID6**: 6-digit OPEID for institution\n",
    "\n",
    "**INSTNM**: Institution name\n",
    "\n",
    "**CITY**: city\n",
    "\n",
    "**STABBR**: State postcode\n",
    "\n",
    "**INSTURL**: URL for instution's homepage\n",
    "\n",
    "**NPCURL**: URL for institution's net price calculator\n",
    "\n",
    "**HCM2**: Schools that are on Heightened Cash Monitoring 2 by the Department of Education\n",
    "\n",
    "**PREDDEG**: Predominant undergraduate degree awarded. Can take 5 values:\n",
    "\n",
    "1. Not classified\n",
    "2. Predominantly certificate-degree granting\n",
    "3. Predominantly associate's-degree granting\n",
    "4. Predominantly bachelor's-degree granting\n",
    "5. Entirely graduate-degree granting\n",
    "\n",
    "**HIGHDEG**: Highest degree awarded. Can take 5 values:\n",
    "\n",
    "1. Non-degree-granting\n",
    "2. Certificate degree\n",
    "3. Associate degree\n",
    "4. Bachelor's degree\n",
    "5. Graduate degree\n",
    "\n",
    "**CONTROL**: Control of institution. Can take 3 values:\n",
    "\n",
    "1. Public\n",
    "2. Private non-profit\n",
    "3. Private for-profit\n",
    "\n",
    "**LOCALE**: Locale of institution. Can take 12 values:\n",
    "1. City: Large (population of 250,000 or more)\n",
    "2. City: Midsize (population of at least 100,000 but less than 250,000)\n",
    "3. City: Small (population less than 100,000)\n",
    "4. Suburb: Large (outside principal city, in urbanized area with population of 250,000 or more)\n",
    "5. Suburb: Midsize (outside principal city, in urbanized area with population of at least 100,000 but less than 250,000)\n",
    "6. Suburb: Small (outside principal city, in urbanized area with population less than 100,000)\n",
    "7. Town: Fringe (in urban cluster up to 10 miles from an urbanized area)\n",
    "8. Town: Distant (in urban cluster more than 10 miles and up to 35 miles from an urbanized area)\n",
    "9. Town: Remote (in urban cluster more than 35 miles from an urbanized area)\n",
    "10. Rural: Fringe (rural territory up to 5 miles from an urbanized area or up to 2.5 miles from an urban cluster)\n",
    "11. Rural: Distant (rural territory more than 5 miles but up to 25 miles from an urbanized area or more than 2.5 and up to 10 miles from an urban cluster)\n",
    "12. Rural: Remote (rural territory more than 25 miles from an urbanized area and more than 10 miles from an urban cluster)\n",
    "\n",
    "**HBCU**: Flag for historically Black College and University.\n",
    "\n",
    "**PBI**: Flag for predominantly black institution.\n",
    "\n",
    "**ANNHI**: Flag for Alaska Native Native Hawaiian serving institution.\n",
    "\n",
    "**TRIBAL**: Flag for tribal college and university\n",
    "\n",
    "**AANAPII**: Flag for Asian American Native American Pacific Islander-serving institution\n",
    "\n",
    "**HSI**: Flag for Hispanic-serving institution\n",
    "\n",
    "**NANTI**: Flag for Native American non-tribal institution\n",
    "\n",
    "**MENONLY**: Flag for men-only college\n",
    "\n",
    "**WOMENONLY**: Flag for women-only college\n",
    "\n",
    "**RELAFFIL**: Religious affiliation of the institution. It can take 65 values:\n",
    "\n",
    "1. Not reported\n",
    "2. Not applicable\n",
    "3. American Evangelical Lutheran Church\n",
    "4. African Methodist Episcopal Zion Church\n",
    "5. Assemblies of God Church\n",
    "6. Brethren Church\n",
    "7. Roman Catholic\n",
    "8. Wisconsin Evangelical Lutheran Synod\n",
    "9. Christ and Missionary Alliance Church\n",
    "10. Christian Reformed Church\n",
    "11. Evangelical Congregational Church\n",
    "12. Evangelical Covenant Church of America\n",
    "13. Evangelical Free Church of America\n",
    "14. Evangelical Lutheran Church\n",
    "15. International United Pentecostal Church\n",
    "16. Free Will Baptist Church\n",
    "17. Interdenominational\n",
    "18. Mennonite Brethren Church\n",
    "19. Moravian Church\n",
    "20. North American Baptist\n",
    "21. Pentecostal Holiness Church\n",
    "22. Christian Churches and Churches of Christ\n",
    "23. Reformed Church in America\n",
    "24. Episcopal Church, Reformed\n",
    "25. African Methodist Episcopal\n",
    "26. American Baptist\n",
    "27. American Lutheran\n",
    "28. Baptist\n",
    "29. Christian Methodist Episcopal\n",
    "30. Church of God\n",
    "31. Church of Brethren\n",
    "32. Church of the Nazarene\n",
    "33. Cumberland Presbyterian\n",
    "34. Christian Church (Disciples of Christ)\n",
    "35. Free Methodist\n",
    "36. Friends\n",
    "37. Presbyterian Church (USA)\n",
    "38. Lutheran Church in America\n",
    "39. Lutheran Church - Missouri Synod\n",
    "40. Mennonite Church\n",
    "41. United Methodist\n",
    "42. Protestant Episcopal\n",
    "43. Churches of Christ\n",
    "44. Southern Baptist\n",
    "45. United Church of Christ\n",
    "46. Protestant, not specified\n",
    "47. Multiple Protestant Denomination\n",
    "48. Other Protestant\n",
    "49. Jewish\n",
    "50. Reformed Presbyterian Church\n",
    "51. United Brethren Church\n",
    "52. Missionary Church Inc\n",
    "53. Undenominational\n",
    "54. Wesleyan\n",
    "55. Greek Orthodox\n",
    "56. Russian Orthodox\n",
    "57. Unitarian Universalist\n",
    "58. Latter Day Saints (Mormon Church)\n",
    "59. Seventh Day Adventists\n",
    "60. The Presbyterian Church in America\n",
    "61. Other (none of the above)\n",
    "62. Original Free Will Baptist\n",
    "63. Ecumenical Christian\n",
    "64. Evangelical Christian\n",
    "65. Presbyterian\n",
    "\n",
    "\n",
    "**SATVR25**: 25th percentile of SAT scores at the institution (critical reading)\n",
    "\n",
    "**SATVR75**: 75th percentile of SAT scores at the institution (critical reading)\n",
    "\n",
    "**SATMT25**: 25th percentile of SAT scores at the institution (math)\n",
    "\n",
    "**SATMT75**: 75th percentile of SAT scores at the institution (math)\n",
    "\n",
    "**SATWR25**: 25th percentile of SAT scores at the institution (writing)\n",
    "\n",
    "**SATWR75**: 75th percentile of SAT scores at the institution (writing)\n",
    "\n",
    "**SATVRMID**: Midpoint of SAT scores at the institution (critical reading)\n",
    "\n",
    "**SATMTMID**: Midpoint of SAT scores at the institution (math)\n",
    "\n",
    "**SATWRMID**: Midpoint of SAT scores at the institution (writing)\n",
    "\n",
    "**ACTCM25**: 25th percentile of the ACT cumulative score\n",
    "\n",
    "**ACTCM75**: 75th percentile of the ACT cumulative score\n",
    "\n",
    "**ACTEN25**: 25th percentile of the ACT English score\n",
    "\n",
    "**ACTEN75**: 75th percentile of the ACT English score\n",
    "\n",
    "**ACTMT25**: 25th percentile of the ACT math score\n",
    "\n",
    "**ACTMT75**: 75th percentile of the ACT math score\n",
    "\n",
    "**ACTWR25**: 25th percentile of the ACT writing score\n",
    "\n",
    "**ACTWR75**: 75th percentile of the ACT writing score\n",
    "\n",
    "**ACTCMMID**: Midpoint of the ACT cumulative score\n",
    "\n",
    "**ACTENMID**: Midpoint of the ACT English score\n",
    "\n",
    "**ACTMTMID**: Midpoint of the ACT math score\n",
    "\n",
    "**ACTWRMID**: Midpoint of the ACT writing score\n",
    "\n",
    "**SAT_AVG**: Average SAT equivalent score of students admitted\n",
    "\n",
    "**SAT_AVG_ALL**:Average SAT equivalent score of students admitted for all campuses rolled up to the 6-digit OPE ID\n",
    "\n",
    "**PCIP01**: Percentage of degrees awarded in Agriculture, Agriculture Operations, And Related Sciences.\n",
    "\n",
    "**PCIP03**: Percentage of degrees awarded in Natural Resources And Conservation.\n",
    "\n",
    "**PCIP04**: Percentage of degrees awarded in Architecture And Related Services.\n",
    "\n",
    "**PCIP05**: Percentage of degrees awarded in Area, Ethnic, Cultural, Gender, And Group Studies.\n",
    "\n",
    "**PCIP09**: Percentage of degrees awarded in Communication, Journalism, And Related Programs.\n",
    "\n",
    "**PCIP10**: Percentage of degrees awarded in Communications Technologies/Technicians And Support Services.\n",
    "\n",
    "**PCIP11**: Percentage of degrees awarded in Computer And Information Sciences And Support Services.\n",
    "\n",
    "**PCIP12**: Percentage of degrees awarded in Personal And Culinary Services.\n",
    "\n",
    "**PCIP13**: Percentage of degrees awarded in Education.\n",
    "\n",
    "**PCIP14**: Percentage of degrees awarded in Engineering.\n",
    "\n",
    "**PCIP15**: Percentage of degrees awarded in Engineering Technologies And Engineering-Related Fields.\n",
    "\n",
    "**PCIP16**: Percentage of degrees awarded in Foreign Languages, Literatures, And Linguistics.\n",
    "\n",
    "**PCIP19**: Percentage of degrees awarded in Family And Consumer Sciences/Human Sciences.\n",
    "\n",
    "**PCIP22**: Percentage of degrees awarded in Legal Professions And Studies.\n",
    "\n",
    "**PCIP23**: Percentage of degrees awarded in English Language And Literature/Letters.\n",
    "\n",
    "**PCIP24**: Percentage of degrees awarded in Liberal Arts And Sciences, General Studies And Humanities.\n",
    "\n",
    "**PCIP25**: Percentage of degrees awarded in Library Science.\n",
    "\n",
    "**PCIP26**: Percentage of degrees awarded in Biological And Biomedical Sciences.\n",
    "\n",
    "**PCIP27**: Percentage of degrees awarded in Mathematics And Statistics.\n",
    "\n",
    "**PCIP29**: Percentage of degrees awarded in Military Technologies And Applied Sciences.\n",
    "\n",
    "**PCIP30**: Percentage of degrees awarded in Multi/Interdisciplinary Studies.\n",
    "\n",
    "**PCIP31**: Percentage of degrees awarded in Parks, Recreation, Leisure, And Fitness Studies.\n",
    "\n",
    "**PCIP38**: Percentage of degrees awarded in Philosophy And Religious Studies.\n",
    "\n",
    "**PCIP39**: Percentage of degrees awarded in Theology And Religious Vocations.\n",
    "\n",
    "**PCIP40**: Percentage of degrees awarded in Physical Sciences.\n",
    "\n",
    "**PCIP41**: Percentage of degrees awarded in Science Technologies/Technicians.\n",
    "\n",
    "**PCIP42**: Percentage of degrees awarded in Psychology.\n",
    "\n",
    "**PCIP43**: Percentage of degrees awarded in Homeland Security, Law Enforcement, Firefighting And Related Protective \n",
    "Services.\n",
    "\n",
    "**PCIP44**: Percentage of degrees awarded in Public Administration And Social Service Professions.\n",
    "\n",
    "**PCIP45**: Percentage of degrees awarded in Social Sciences.\n",
    "\n",
    "**PCIP46**: Percentage of degrees awarded in Construction Trades.\n",
    "\n",
    "**PCIP47**: Percentage of degrees awarded in Mechanic And Repair Technologies/Technicians.\n",
    "\n",
    "**PCIP48**: Percentage of degrees awarded in Precision Production.\n",
    "\n",
    "**PCIP49**: Percentage of degrees awarded in Transportation And Materials Moving.\n",
    "\n",
    "**PCIP50**: Percentage of degrees awarded in Visual And Performing Arts.\n",
    "\n",
    "**PCIP51**: Percentage of degrees awarded in Health Professions And Related Programs.\n",
    "\n",
    "**PCIP52**: Percentage of degrees awarded in Business, Management, Marketing, And Related Support Services.\n",
    "\n",
    "**PCIP54**: Percentage of degrees awarded in History.\n",
    "\n",
    "**DISTANCEONLY**: Flag for distance-education-only education\n",
    "\n",
    "**UGDS**: Enrollment of undergraduate certificate/degree-seeking students\n",
    "\n",
    "**UGDS_WHITE**: Total share of enrollment of undergraduate degree-seeking students who are white\n",
    "\n",
    "**UGDS_BLACK**: Total share of enrollment of undergraduate degree-seeking students who are black\n",
    "\n",
    "**UGDS_HISP**: Total share of enrollment of undergraduate degree-seeking students who are Hispanic\n",
    "\n",
    "**UGDS_ASIAN**: Total share of enrollment of undergraduate degree-seeking students who are Asian\n",
    "\n",
    "**UGDS_AIAN**: Total share of enrollment of undergraduate degree-seeking students who are American Indian/Alaska Native\n",
    "\n",
    "**UGDS_NHPI**: Total share of enrollment of undergraduate degree-seeking students who are Native Hawaiian/Pacific \n",
    "Islander\n",
    "\n",
    "**UGDS_2MOR**: Total share of enrollment of undergraduate degree-seeking students who are two or more races\n",
    "\n",
    "**UGDS_NRA**: Total share of enrollment of undergraduate degree-seeking students who are non-resident aliens\n",
    "\n",
    "**UGDS_UNKN**: Total share of enrollment of undergraduate degree-seeking students whose race is unknown\n",
    "\n",
    "**PPTUG_EF**: Share of undergraduate, degree-/certificate-seeking students who are part-time \n",
    "\n",
    "**CURROPER**: Flag for currently operating institution, 0=closed, 1=operating\n",
    "\n",
    "**NPT4_PUB**: Average net price for Title IV institutions (public institutions)\n",
    "\n",
    "**NPT4_PRIV**: Average net price for Title IV institutions (private for-profit and nonprofit institutions)\n",
    "\n",
    "**NPT41_PUB**: Average net price for \\$0-$30,000 family income (public institutions)\n",
    "\n",
    "**NPT42_PUB**: Average net price for \\$30,001-$48,000 family income (public institutions)\n",
    "\n",
    "**NPT43_PUB**: Average net price for \\$48,001-$75,000 family income (public institutions)\n",
    "\n",
    "**NPT44_PUB**: Average net price for \\$75,001-$110,000 family income (public institutions)\n",
    "\n",
    "**NPT45_PUB**: Average net price for \\$110,000+ family income (public institutions)\n",
    "\n",
    "**NPT41_PRIV**: Average net price for \\$0-$30,000 family income (private for-profit and nonprofit institutions)\n",
    "\n",
    "**NPT42_PRIV**: Average net price for \\$30,001-$48,000 family income (private for-profit and nonprofit institutions)\n",
    "\n",
    "**NPT43_PRIV**: Average net price for \\$48,001-$75,000 family income (private for-profit and nonprofit institutions)\n",
    "\n",
    "**NPT44_PRIV**: Average net price for \\$75,001-$110,000 family income (private for-profit and nonprofit institutions)\n",
    "\n",
    "**NPT45_PRIV**: Average net price for \\$110,000+ family income (private for-profit and nonprofit institutions)\n",
    "\n",
    "**PCTPELL**: Percentage of undergraduates who receive a Pell Grant\n",
    "\n",
    "**PCTFLOAN**: Percent of all undergraduate students receiving a federal student loan\n",
    "\n",
    "**UG25ABV**: Percentage of undergraduates aged 25 and above\n",
    "\n",
    "**MD_EARN_WNE_P10**: Median earnings of students working and not enrolled 10 years after entry\n",
    "\n",
    "**GT_25K_P6**: Share of students earning over $25,000/year (threshold earnings) 6 years after entry\n",
    "\n",
    "**GRAD_DEBT_MDN_SUPP**: Median debt of completers, suppressed for n=30\n",
    "\n",
    "**GRAD_DEBT_MDN10YR_SUPP**: Median debt of completers expressed in 10-year monthly payments, suppressed for n=30\n",
    "\n",
    "**RPY_3YR_RT_SUPP**: 3-year repayment rate, suppressed for n=30\n",
    "\n",
    "## Preparing the data\n",
    "\n",
    "Let us check the quality of data. But before doing this, let us remove the features in which we are not interested.\n",
    "\n",
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7593 entries, 0 to 7592\n",
      "Data columns (total 3 columns):\n",
      "OPEID     7593 non-null int64\n",
      "OPEID6    7593 non-null int64\n",
      "UNITID    7593 non-null int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 178.0 KB\n"
     ]
    }
   ],
   "source": [
    "df[[\"OPEID\", \"OPEID6\", \"UNITID\"]].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for duplicates in these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in OPEID is 30\n",
      "Number of duplicates in OPEID6 is 1994\n",
      "Number of duplicates in UNITID is 0\n"
     ]
    }
   ],
   "source": [
    "for col in ['OPEID', 'OPEID6', 'UNITID']:\n",
    "    print(\"Number of duplicates in {} is {}\".format(col,df[col].duplicated().sum()))#[df['OPEID'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to use `UNITID` as identifiers for every row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing irrelevant identifiers\n",
    "\n",
    "df.drop([\"OPEID\", \"OPEID6\"],\n",
    "       axis = 1,\n",
    "       inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SAT and ACT scores we have 25 percentile, 75 percentile and mid-point values.We will only use the midpoint values for calculation to avoid curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing 25 and 75 percentile marks\n",
    "\n",
    "df.drop(['SATVR25', 'SATVR75',\n",
    "        'SATMT25', 'SATMT75',\n",
    "        'SATWR25', 'SATWR75',\n",
    "        'ACTCM25', 'ACTCM75',\n",
    "        'ACTEN25', 'ACTEN75',\n",
    "        'ACTMT25', 'ACTMT75',\n",
    "        'ACTWR25', 'ACTWR75'],\n",
    "       axis = 1,\n",
    "       inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two columns for SAT Average: `SAT_AVG` and `SAT_AVG_ALL`. Since we have removed `OPEID` column, we will be using `SAT_AVG` column because it provides overall stats, rather than averages based on `OPEID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['SAT_AVG_ALL'],\n",
    "       axis = 1,\n",
    "       inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have average net price for public and private institutions. We will remove the average price based on different family income levels and use the overall average net price for public and private institutions. We am removing columns related to family income levels because We don't that family income is a university level factor. This is a factor based on student level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing prices based on family income\n",
    "\n",
    "df.drop(['NPT41_PUB', 'NPT42_PUB', 'NPT43_PUB', 'NPT44_PUB', 'NPT45_PUB',\n",
    "         'NPT41_PRIV', 'NPT42_PRIV', 'NPT43_PRIV', 'NPT44_PRIV', 'NPT45_PRIV'],\n",
    "       axis = 1,\n",
    "       inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see other variables which have many distinct values and need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7593 entries, 0 to 7592\n",
      "Data columns (total 96 columns):\n",
      "UNITID                    7593 non-null int64\n",
      "INSTNM                    7593 non-null object\n",
      "CITY                      7593 non-null object\n",
      "STABBR                    7593 non-null object\n",
      "INSTURL                   7573 non-null object\n",
      "NPCURL                    7075 non-null object\n",
      "HCM2                      7593 non-null int64\n",
      "PREDDEG                   7593 non-null int64\n",
      "HIGHDEG                   7593 non-null int64\n",
      "CONTROL                   7593 non-null int64\n",
      "LOCALE                    7147 non-null float64\n",
      "HBCU                      7147 non-null float64\n",
      "PBI                       7147 non-null float64\n",
      "ANNHI                     7147 non-null float64\n",
      "TRIBAL                    7147 non-null float64\n",
      "AANAPII                   7147 non-null float64\n",
      "HSI                       7147 non-null float64\n",
      "NANTI                     7147 non-null float64\n",
      "MENONLY                   7147 non-null float64\n",
      "WOMENONLY                 7147 non-null float64\n",
      "RELAFFIL                  909 non-null float64\n",
      "SATVRMID                  1193 non-null float64\n",
      "SATMTMID                  1202 non-null float64\n",
      "SATWRMID                  733 non-null float64\n",
      "ACTCMMID                  1234 non-null float64\n",
      "ACTENMID                  1093 non-null float64\n",
      "ACTMTMID                  1094 non-null float64\n",
      "ACTWRMID                  325 non-null float64\n",
      "SAT_AVG                   1278 non-null float64\n",
      "PCIP01                    6854 non-null float64\n",
      "PCIP03                    6854 non-null float64\n",
      "PCIP04                    6854 non-null float64\n",
      "PCIP05                    6854 non-null float64\n",
      "PCIP09                    6854 non-null float64\n",
      "PCIP10                    6854 non-null float64\n",
      "PCIP11                    6854 non-null float64\n",
      "PCIP12                    6854 non-null float64\n",
      "PCIP13                    6854 non-null float64\n",
      "PCIP14                    6854 non-null float64\n",
      "PCIP15                    6854 non-null float64\n",
      "PCIP16                    6854 non-null float64\n",
      "PCIP19                    6854 non-null float64\n",
      "PCIP22                    6854 non-null float64\n",
      "PCIP23                    6854 non-null float64\n",
      "PCIP24                    6854 non-null float64\n",
      "PCIP25                    6854 non-null float64\n",
      "PCIP26                    6854 non-null float64\n",
      "PCIP27                    6854 non-null float64\n",
      "PCIP29                    6854 non-null float64\n",
      "PCIP30                    6854 non-null float64\n",
      "PCIP31                    6854 non-null float64\n",
      "PCIP38                    6854 non-null float64\n",
      "PCIP39                    6854 non-null float64\n",
      "PCIP40                    6854 non-null float64\n",
      "PCIP41                    6854 non-null float64\n",
      "PCIP42                    6854 non-null float64\n",
      "PCIP43                    6854 non-null float64\n",
      "PCIP44                    6854 non-null float64\n",
      "PCIP45                    6854 non-null float64\n",
      "PCIP46                    6854 non-null float64\n",
      "PCIP47                    6854 non-null float64\n",
      "PCIP48                    6854 non-null float64\n",
      "PCIP49                    6854 non-null float64\n",
      "PCIP50                    6854 non-null float64\n",
      "PCIP51                    6854 non-null float64\n",
      "PCIP52                    6854 non-null float64\n",
      "PCIP54                    6854 non-null float64\n",
      "DISTANCEONLY              7147 non-null float64\n",
      "UGDS                      6858 non-null float64\n",
      "UGDS_WHITE                6858 non-null float64\n",
      "UGDS_BLACK                6858 non-null float64\n",
      "UGDS_HISP                 6858 non-null float64\n",
      "UGDS_ASIAN                6858 non-null float64\n",
      "UGDS_AIAN                 6858 non-null float64\n",
      "UGDS_NHPI                 6858 non-null float64\n",
      "UGDS_2MOR                 6858 non-null float64\n",
      "UGDS_NRA                  6858 non-null float64\n",
      "UGDS_UNKN                 6858 non-null float64\n",
      "PPTUG_EF                  6831 non-null float64\n",
      "CURROPER                  7593 non-null int64\n",
      "NPT4_PUB                  1906 non-null float64\n",
      "NPT4_PRIV                 4513 non-null float64\n",
      "PCTPELL                   6835 non-null float64\n",
      "RET_FT4                   2245 non-null float64\n",
      "RET_FTL4                  3766 non-null float64\n",
      "RET_PT4                   1392 non-null float64\n",
      "RET_PTL4                  2128 non-null float64\n",
      "PCTFLOAN                  6835 non-null float64\n",
      "UG25ABV                   6788 non-null float64\n",
      "MD_EARN_WNE_P10           6454 non-null object\n",
      "GT_25K_P6                 6454 non-null object\n",
      "GRAD_DEBT_MDN_SUPP        7564 non-null object\n",
      "GRAD_DEBT_MDN10YR_SUPP    7564 non-null object\n",
      "RPY_3YR_RT_SUPP           6666 non-null object\n",
      "C150_L4_POOLED_SUPP       3890 non-null object\n",
      "C150_4_POOLED_SUPP        2505 non-null object\n",
      "dtypes: float64(78), int64(6), object(12)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am removing the **Categorical Columns ( dtype object )** which have many levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['INSTNM', \"CITY\", \"INSTURL\", \"NPCURL\", \"STABBR\", \"RELAFFIL\"],\n",
    "       axis = 1,\n",
    "       inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am keeping categorical columns which have less than 10 levels.\n",
    "\n",
    "`INSTNM`, `INSTURL` & `NPCURL` are identifiers for colleges in offline or online media and are not university level factors which affect education's status.\n",
    "\n",
    "`STABBR`, `CITY` & `RELAFFIL` columns have been removed because they had too many levels to be considered.\n",
    "\n",
    "Calling `.info()` method shows data type _\"object\"_ for last few columns. This is because some values in these columns are \"**PrivacySuppressed**\".\n",
    "\n",
    "Checking for null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOCALE</th>\n",
       "      <th>HBCU</th>\n",
       "      <th>PBI</th>\n",
       "      <th>ANNHI</th>\n",
       "      <th>TRIBAL</th>\n",
       "      <th>AANAPII</th>\n",
       "      <th>HSI</th>\n",
       "      <th>NANTI</th>\n",
       "      <th>MENONLY</th>\n",
       "      <th>WOMENONLY</th>\n",
       "      <th>...</th>\n",
       "      <th>RET_PTL4</th>\n",
       "      <th>PCTFLOAN</th>\n",
       "      <th>UG25ABV</th>\n",
       "      <th>MD_EARN_WNE_P10</th>\n",
       "      <th>GT_25K_P6</th>\n",
       "      <th>GRAD_DEBT_MDN_SUPP</th>\n",
       "      <th>GRAD_DEBT_MDN10YR_SUPP</th>\n",
       "      <th>RPY_3YR_RT_SUPP</th>\n",
       "      <th>C150_L4_POOLED_SUPP</th>\n",
       "      <th>C150_4_POOLED_SUPP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8159</td>\n",
       "      <td>0.0877</td>\n",
       "      <td>29900</td>\n",
       "      <td>0.453</td>\n",
       "      <td>35000</td>\n",
       "      <td>361.891446885773</td>\n",
       "      <td>0.2458495231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5218</td>\n",
       "      <td>0.2363</td>\n",
       "      <td>40200</td>\n",
       "      <td>0.669</td>\n",
       "      <td>21500</td>\n",
       "      <td>222.304745944118</td>\n",
       "      <td>0.5199110572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>0.8571</td>\n",
       "      <td>40100</td>\n",
       "      <td>0.658</td>\n",
       "      <td>23000</td>\n",
       "      <td>237.814379382079</td>\n",
       "      <td>0.2331002331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PrivacySuppressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4589</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>45600</td>\n",
       "      <td>0.685</td>\n",
       "      <td>23500</td>\n",
       "      <td>242.984257194733</td>\n",
       "      <td>0.5490029699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7692</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>26700</td>\n",
       "      <td>0.393</td>\n",
       "      <td>32091</td>\n",
       "      <td>331.813097771753</td>\n",
       "      <td>0.1963538553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LOCALE  HBCU  PBI  ANNHI  TRIBAL  AANAPII  HSI  NANTI  MENONLY  WOMENONLY  \\\n",
       "0    12.0   1.0  0.0    0.0     0.0      0.0  0.0    0.0      0.0        0.0   \n",
       "1    12.0   0.0  0.0    0.0     0.0      0.0  0.0    0.0      0.0        0.0   \n",
       "2    12.0   0.0  1.0    0.0     0.0      0.0  0.0    0.0      0.0        0.0   \n",
       "3    12.0   0.0  0.0    0.0     0.0      0.0  0.0    0.0      0.0        0.0   \n",
       "4    12.0   1.0  0.0    0.0     0.0      0.0  0.0    0.0      0.0        0.0   \n",
       "\n",
       "          ...          RET_PTL4  PCTFLOAN  UG25ABV  MD_EARN_WNE_P10  \\\n",
       "0         ...               NaN    0.8159   0.0877            29900   \n",
       "1         ...               NaN    0.5218   0.2363            40200   \n",
       "2         ...               NaN    0.8781   0.8571            40100   \n",
       "3         ...               NaN    0.4589   0.2255            45600   \n",
       "4         ...               NaN    0.7692   0.0974            26700   \n",
       "\n",
       "   GT_25K_P6  GRAD_DEBT_MDN_SUPP  GRAD_DEBT_MDN10YR_SUPP  RPY_3YR_RT_SUPP  \\\n",
       "0      0.453               35000        361.891446885773     0.2458495231   \n",
       "1      0.669               21500        222.304745944118     0.5199110572   \n",
       "2      0.658               23000        237.814379382079     0.2331002331   \n",
       "3      0.685               23500        242.984257194733     0.5490029699   \n",
       "4      0.393               32091        331.813097771753     0.1963538553   \n",
       "\n",
       "   C150_L4_POOLED_SUPP  C150_4_POOLED_SUPP  \n",
       "0                  NaN              0.3303  \n",
       "1                  NaN              0.5504  \n",
       "2                  NaN   PrivacySuppressed  \n",
       "3                  NaN              0.4776  \n",
       "4                  NaN              0.2663  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,df.isnull().any()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**\n",
    "\n",
    "There are two types of invalid entries here. First is the `Nan` entry and another is `PrivacySuppressed`.\n",
    "\n",
    "We will first convert `PrivacySuppressed` to null value and then replace all the null values accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert \"PrivacySuppressed\" to NaN\n",
    "df.replace('PrivacySuppressed', np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNITID</th>\n",
       "      <th>HCM2</th>\n",
       "      <th>PREDDEG</th>\n",
       "      <th>HIGHDEG</th>\n",
       "      <th>CONTROL</th>\n",
       "      <th>LOCALE</th>\n",
       "      <th>HBCU</th>\n",
       "      <th>PBI</th>\n",
       "      <th>ANNHI</th>\n",
       "      <th>TRIBAL</th>\n",
       "      <th>...</th>\n",
       "      <th>RET_PTL4</th>\n",
       "      <th>PCTFLOAN</th>\n",
       "      <th>UG25ABV</th>\n",
       "      <th>MD_EARN_WNE_P10</th>\n",
       "      <th>GT_25K_P6</th>\n",
       "      <th>GRAD_DEBT_MDN_SUPP</th>\n",
       "      <th>GRAD_DEBT_MDN10YR_SUPP</th>\n",
       "      <th>RPY_3YR_RT_SUPP</th>\n",
       "      <th>C150_L4_POOLED_SUPP</th>\n",
       "      <th>C150_4_POOLED_SUPP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100654</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8159</td>\n",
       "      <td>0.0877</td>\n",
       "      <td>29900</td>\n",
       "      <td>0.453</td>\n",
       "      <td>35000</td>\n",
       "      <td>361.891446885773</td>\n",
       "      <td>0.2458495231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100663</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5218</td>\n",
       "      <td>0.2363</td>\n",
       "      <td>40200</td>\n",
       "      <td>0.669</td>\n",
       "      <td>21500</td>\n",
       "      <td>222.304745944118</td>\n",
       "      <td>0.5199110572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100690</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>0.8571</td>\n",
       "      <td>40100</td>\n",
       "      <td>0.658</td>\n",
       "      <td>23000</td>\n",
       "      <td>237.814379382079</td>\n",
       "      <td>0.2331002331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100706</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4589</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>45600</td>\n",
       "      <td>0.685</td>\n",
       "      <td>23500</td>\n",
       "      <td>242.984257194733</td>\n",
       "      <td>0.5490029699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100724</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7692</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>26700</td>\n",
       "      <td>0.393</td>\n",
       "      <td>32091</td>\n",
       "      <td>331.813097771753</td>\n",
       "      <td>0.1963538553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UNITID  HCM2  PREDDEG  HIGHDEG  CONTROL  LOCALE  HBCU  PBI  ANNHI  TRIBAL  \\\n",
       "0  100654     0        3        4        1    12.0   1.0  0.0    0.0     0.0   \n",
       "1  100663     0        3        4        1    12.0   0.0  0.0    0.0     0.0   \n",
       "2  100690     0        3        4        2    12.0   0.0  1.0    0.0     0.0   \n",
       "3  100706     0        3        4        1    12.0   0.0  0.0    0.0     0.0   \n",
       "4  100724     0        3        4        1    12.0   1.0  0.0    0.0     0.0   \n",
       "\n",
       "          ...          RET_PTL4  PCTFLOAN  UG25ABV  MD_EARN_WNE_P10  \\\n",
       "0         ...               NaN    0.8159   0.0877            29900   \n",
       "1         ...               NaN    0.5218   0.2363            40200   \n",
       "2         ...               NaN    0.8781   0.8571            40100   \n",
       "3         ...               NaN    0.4589   0.2255            45600   \n",
       "4         ...               NaN    0.7692   0.0974            26700   \n",
       "\n",
       "   GT_25K_P6  GRAD_DEBT_MDN_SUPP  GRAD_DEBT_MDN10YR_SUPP  RPY_3YR_RT_SUPP  \\\n",
       "0      0.453               35000        361.891446885773     0.2458495231   \n",
       "1      0.669               21500        222.304745944118     0.5199110572   \n",
       "2      0.658               23000        237.814379382079     0.2331002331   \n",
       "3      0.685               23500        242.984257194733     0.5490029699   \n",
       "4      0.393               32091        331.813097771753     0.1963538553   \n",
       "\n",
       "   C150_L4_POOLED_SUPP  C150_4_POOLED_SUPP  \n",
       "0                  NaN              0.3303  \n",
       "1                  NaN              0.5504  \n",
       "2                  NaN                 NaN  \n",
       "3                  NaN              0.4776  \n",
       "4                  NaN              0.2663  \n",
       "\n",
       "[5 rows x 90 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7593 entries, 0 to 7592\n",
      "Data columns (total 7 columns):\n",
      "MD_EARN_WNE_P10           5682 non-null object\n",
      "GT_25K_P6                 5840 non-null object\n",
      "GRAD_DEBT_MDN_SUPP        6024 non-null object\n",
      "GRAD_DEBT_MDN10YR_SUPP    6024 non-null object\n",
      "RPY_3YR_RT_SUPP           6171 non-null object\n",
      "C150_L4_POOLED_SUPP       3718 non-null object\n",
      "C150_4_POOLED_SUPP        2371 non-null object\n",
      "dtypes: object(7)\n",
      "memory usage: 415.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df[df.columns[-7:]].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting `PrivacySuppressed` to `NaN`, pandas still treats them as _object_ data type. Below is a table showing correct data type of these columns(from metadata.xlsx file):\n",
    "\n",
    "| Column| Data type |\n",
    "|------|------|\n",
    "|MD_EARN_WNE_P10| integer|\n",
    "|GT_25K_P6|float|\n",
    "|GRAD_DEBT_MDN_SUPP|float|\n",
    "|GRAD_DEBT_MDN10YR_SUPP|float|\n",
    "|RPY_3YR_RT_SUPP|float|\n",
    "|C150_L4_POOLED_SUPP|float|\n",
    "|C150_4_POOLED_SUPP|float|\n",
    "\n",
    "Converting `MD_EARN_WNE_P10` to integer data type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the below line will throw an error because NaN cannot be converted to integer\n",
    "#df['MD_EARN_WNE_P10'] = df['MD_EARN_WNE_P10'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will convert `MD_EARN_WNE_P10` to _float_ type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of the last few columns which have data type string\n",
    "str_cols = df[df.columns[-7:]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting columns' data type from string to float\n",
    "for col in str_cols:\n",
    "    df[col] = df[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7593 entries, 0 to 7592\n",
      "Data columns (total 7 columns):\n",
      "MD_EARN_WNE_P10           5682 non-null float64\n",
      "GT_25K_P6                 5840 non-null float64\n",
      "GRAD_DEBT_MDN_SUPP        6024 non-null float64\n",
      "GRAD_DEBT_MDN10YR_SUPP    6024 non-null float64\n",
      "RPY_3YR_RT_SUPP           6171 non-null float64\n",
      "C150_L4_POOLED_SUPP       3718 non-null float64\n",
      "C150_4_POOLED_SUPP        2371 non-null float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 415.3 KB\n"
     ]
    }
   ],
   "source": [
    "# verifying\n",
    "df[df.columns[-7:]].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before applying median imputation, we need to check for some other categorical columns which might be important for analysis. There are some categorical levels which cannot be ignored while building the model. Those columns are:\n",
    "1. PREDDEG\n",
    "2. HIGHDEG\n",
    "3. CONTROL\n",
    "4. LOCALE(contains 12 levels but I will reduce them to 4 levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['PREDDEG']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['HIGHDEG']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['CONTROL']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## running the line below will throw an error because there are two types of null values in LOCALE column: nan and -3\n",
    "#plt.hist(df['LOCALE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12., 13., 32., 31., 41., 21., 43., 42., 11., 23., 33., 22., -3.,\n",
       "       nan])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['LOCALE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to facilitate our visualisation of `LOCALE` column ,we will exclude NaNs and -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df[(df['LOCALE'] != -3) & (df['LOCALE'].notnull())]['LOCALE']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will create dummy variables for these 4 categorical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for PREDDEG column\n",
    "df[['pred_deg.not_classified', 'pred_deg.certificate_degree', 'pred_deg.associates_degree',\n",
    "    'pred_deg.bachelors_degree', 'pred_deg.entire_graduate']] = pd.get_dummies(df['PREDDEG'])\n",
    "\n",
    "\n",
    "\n",
    "# for CONTROL column\n",
    "df[['Public', 'Private nonprofit', 'Private for-profit']] = pd.get_dummies(df.CONTROL)\n",
    "\n",
    "\n",
    "\n",
    "# for HIGHDEG column\n",
    "df[['Non-degree-granting', 'Certificate degree',\n",
    "    'Associate Degree', 'Bachelors Degree', 'Graduate Degree']] = pd.get_dummies(df.HIGHDEG)\n",
    "\n",
    "\n",
    "\n",
    "# for LOCALE column\n",
    "loc_new = []\n",
    "\n",
    "for loc in df.LOCALE:\n",
    "    if loc in [11, 12, 13]:\n",
    "        loc_new.append('CITY')\n",
    "    elif loc in [21, 22, 23]:\n",
    "        loc_new.append('SUBURB')\n",
    "    elif loc in [31, 32, 33]:\n",
    "        loc_new.append('TOWN')\n",
    "    elif loc in [41, 42, 43]:\n",
    "        loc_new.append('RURAL')\n",
    "    else:\n",
    "        loc_new.append(np.nan)\n",
    "\n",
    "# replacing the old column        \n",
    "df['LOCALE'] = loc_new\n",
    "\n",
    "# creating dummy variable\n",
    "df[['CITY', 'RURAL', 'SUBURB', 'TOWN']] = pd.get_dummies(df.LOCALE)\n",
    "\n",
    "# removing the original columns\n",
    "df.drop(['PREDDEG', 'CONTROL', 'HIGHDEG', 'LOCALE'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing null values in numerical columns with their respective median\n",
    "for col in df[df.columns]: \n",
    "    df[col].fillna(value = df[col].median(),\n",
    "                  inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names are not user-friendly. I am renaming them for easy readability. The new names are developer friendly names taken from metadata.xlsx file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'HCM2':'under_investigation',\n",
    "           'HBCU':'minority_serving.historically_black',\n",
    "           'PBI': 'minority_serving.predominantly_black',\n",
    "           'ANNHI': 'minority_serving.annh',\n",
    "           'TRIBAL': 'minority_serving.tribal',\n",
    "           'AANAPII': 'minority_serving.aanapi',\n",
    "           'HSI': 'minority_serving.hispanic',\n",
    "           'NANTI': 'minority_serving.nant',\n",
    "           'MENONLY': 'men_only',\n",
    "           'WOMENONLY': 'women_only',\n",
    "           'SATVRMID': 'sat_scores.midpoint.critical_reading',\n",
    "           'SATMTMID': 'sat_scores.midpoint.math',\n",
    "           'SATWRMID': 'sat_scores.midpoint.writing',\n",
    "           'ACTCMMID': 'act_scores.midpoint.cumulative',\n",
    "           'ACTENMID': 'act_scores.midpoint.english',\n",
    "           'ACTMTMID': 'act_scores.midpoint.math',\n",
    "           'ACTWRMID': 'act_scores.midpoint.writing',\n",
    "           'SAT_AVG': 'sat_scores.average.overall',\n",
    "           'PCIP01': 'program_percentage.agriculture',\n",
    "           'PCIP03': 'program_percentage.resources',\n",
    "           'PCIP04': 'program_percentage.architecture',\n",
    "           'PCIP05': 'program_percentage.ethnic_cultural_gender',\n",
    "           'PCIP09': 'program_percentage.communication',\n",
    "           'PCIP10': 'program_percentage.communications_technology',\n",
    "           'PCIP11': 'program_percentage.computer',\n",
    "           'PCIP12': 'program_percentage.personal_culinary',\n",
    "           'PCIP13': 'program_percentage.education',\n",
    "           'PCIP14': 'program_percentage.engineering',\n",
    "           'PCIP15': 'program_percentage.engineering_technology',\n",
    "           'PCIP16': 'program_percentage.language',\n",
    "           'PCIP19': 'program_percentage.family_consumer_service',\n",
    "           'PCIP22': 'program_percentage.legal',\n",
    "           'PCIP23': 'program_percentage.english',\n",
    "           'PCIP24': 'program_percentage.humanities',\n",
    "           'PCIP25': 'program_percentage.libraries',\n",
    "           'PCIP26': 'program_percentage.biological',\n",
    "           'PCIP27': 'program_percentage.mathematics',\n",
    "           'PCIP29': 'program_percentage.military',\n",
    "           'PCIP30': 'program_percentage.multidiscipline',\n",
    "           'PCIP31': 'program_percentage.parks_recreation_fitness',\n",
    "           'PCIP38': 'program_percentage.philosophy_religious',\n",
    "           'PCIP39': 'program_percentage.theology_religious_vocation',\n",
    "           'PCIP40': 'program_percentage.physical_science',\n",
    "           'PCIP41': 'program_percentage.science_technology',\n",
    "           'PCIP42': 'program_percentage.psychology',\n",
    "           'PCIP43': 'program_percentage.security_law_enforcement',\n",
    "           'PCIP44': 'program_percentage.public_administration_social_service',\n",
    "           'PCIP45': 'program_percentage.social_science',\n",
    "           'PCIP46': 'program_percentage.construction',\n",
    "           'PCIP47': 'program_percentage.mechanic_repair_technology',\n",
    "           'PCIP48': 'program_percentage.precision_production',\n",
    "           'PCIP49': 'program_percentage.transportation',\n",
    "           'PCIP50': 'program_percentage.visual_performing',\n",
    "           'PCIP51': 'program_percentage.health',\n",
    "           'PCIP52': 'program_percentage.business_marketing',\n",
    "           'PCIP54': 'program_percentage.history',\n",
    "           'DISTANCEONLY': 'online_only',\n",
    "           'UGDS': 'size',\n",
    "           'UGDS_WHITE': 'demographics.race_ethnicity.white',\n",
    "           'UGDS_BLACK': 'demographics.race_ethnicity.black',\n",
    "           'UGDS_HISP': 'demographics.race_ethnicity.hispanic',\n",
    "           'UGDS_ASIAN': 'demographics.race_ethnicity.asian',\n",
    "           'UGDS_AIAN': 'demographics.race_ethnicity.aian',\n",
    "           'UGDS_NHPI': 'demographics.race_ethnicity.nhpi',\n",
    "           'UGDS_2MOR': 'demographics.race_ethnicity.two_or_more',\n",
    "           'UGDS_NRA': 'demographics.race_ethnicity.non_resident_alien',\n",
    "           'UGDS_UNKN': 'demographics.race_ethnicity.unknown',\n",
    "           'PPTUG_EF': 'part_time_share',\n",
    "           'CURROPER': 'operating',\n",
    "           'NPT4_PUB': 'avg_net_price.public',\n",
    "           'NPT4_PRIV': 'avg_net_price.private',\n",
    "           'PCTPELL': 'pell_grant_rate',\n",
    "           'RET_FT4': 'retention_rate.four_year.full_time',\n",
    "           'RET_FTL4': 'retention_rate.lt_four_year.full_time',\n",
    "           'RET_PT4': 'retention_rate.four_year.part_time',\n",
    "           'RET_PTL4': 'retention_rate.lt_four_year.part_time',\n",
    "           'PCTFLOAN': 'federal_loan_rate',\n",
    "           'UG25ABV': 'share_25_older',\n",
    "           'MD_EARN_WNE_P10': '10_years_after_entry.median',\n",
    "           'GT_25K_P6': '6_yrs_after_entry.percent_greater_than_25000',\n",
    "           'GRAD_DEBT_MDN_SUPP': 'median_debt_suppressed.completers.overall',\n",
    "           'GRAD_DEBT_MDN10YR_SUPP': 'median_debt_suppressed.completers.monthly_payments',\n",
    "           'RPY_3YR_RT_SUPP': '3_yr_repayment_suppressed.overall',\n",
    "           'C150_L4_POOLED_SUPP': 'rate_suppressed.lt_four_year_150percent',\n",
    "           'C150_4_POOLED_SUPP': 'rate_suppressed.four_year'},\n",
    "         inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying response variable\n",
    "\n",
    "Our aim is to determine the relevant university level factors which predict the presence of a strong retention and graduation rates.\n",
    "\n",
    "1. For graduation rates we have two variables in our data. Let us see the difference between those two:\n",
    "\n",
    "    1.1. **rate_suppressed.four_year**\n",
    "\n",
    "    Completion rate for first-time, full-time students at four-year institutions (150% of expected time to completion) , pooled in two-year rolling averages and suppressed for small n size.\n",
    "    \n",
    "    1.2. **rate_suppressed.lt_four_year_150percent**\n",
    "    \n",
    "    Completion rate for first-time, full-time students at less-than-four-year institutions (150% of expected time to completion) , pooled in two-year rolling averages and suppressed for small n size\n",
    "\n",
    "    We will be making predictions for 4-year institutions.\n",
    "    \n",
    "2. For retention rates we have four variables in our data. Let us see the difference between them:\n",
    "\n",
    "    2.1. **retention_rate.four_year.full_time**\n",
    "\n",
    "    First-time, full-time student retention rate at four-year institutions.\n",
    "\n",
    "    2.2. **retention_rate.lt_four_year.full_time**\n",
    "\n",
    "    First-time, full-time student retention rate at less-than-four-year institutions.\n",
    "\n",
    "    2.3. **retention_rate.four_year.part_time**\n",
    "\n",
    "    First-time, part-time student retention rate at four-year institutions\n",
    "\n",
    "    2.4. **retention_rate.lt_four_year.part_time**\n",
    "\n",
    "    First-time, part-time student retention rate at four-year institutions\n",
    "\n",
    "    \n",
    "Retention rate is for full-time students and we are making predictions for 4-year institutions. So, our target variable is **retention_rate.four_year.full_time**.\n",
    "    \n",
    "    \n",
    "So there are 2 response variables:\n",
    "1. **rate_suppressed.four_year**\n",
    "2. **retention_rate.four_year.full_time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting data to get target variables\n",
    "target = df[['rate_suppressed.four_year', 'retention_rate.four_year.full_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting data to get features\n",
    "\n",
    "features = df.drop(['UNITID',\n",
    "                   'rate_suppressed.four_year', 'rate_suppressed.lt_four_year_150percent',\n",
    "                   'retention_rate.four_year.full_time', 'retention_rate.lt_four_year.full_time',\n",
    "                   'retention_rate.four_year.part_time', 'retention_rate.lt_four_year.part_time'],\n",
    "                  axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us see the distribution of target labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# graduation rate\n",
    "grad = df['rate_suppressed.four_year']\n",
    "plt.hist(grad);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAAEcdJREFUeJzt3X+s3fVdx/Hna3RM3eZaxl1D2s5irD9Qs43cAMvM3FYthZmVxEkwKpU0NlFcpi4q0z+qsCUjxs2RTLRKtSwqQ/xBoyg2HcuiEcZFJg7Y5I7BaIX1bi3VSbbJ9vaP8yleWe/uue2553D5PB/Jzfl839/P+X4/n972vu73x/k2VYUkqT8vmPQAJEmTYQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOrVq0gP4Rs4888zauHHjpIchSSvKPffc8/mqmlqs33M6ADZu3MjMzMykhyFJK0qSR4fp5ykgSeqUASBJnTIAJKlTBoAkdWqoAEiyOsktST6Z5MEkr01yRpL9SR5qr2ta3yS5LslskvuSnDtvO9tb/4eSbF+uSUmSFjfsEcD7gb+vqu8GXgU8CFwFHKiqTcCBtgxwEbCpfe0ErgdIcgawCzgfOA/YdTw0JEnjt2gAJHkZ8HrgBoCq+kpVPQlsA/a2bnuBS1p7G3BjDdwJrE5yFnAhsL+qjlTVUWA/sHWks5EkDW2YI4CzgTngj5Lcm+QPk7wYWFtVj7c+TwBrW3sd8Ni89x9stYXq/0+SnUlmkszMzc0tbTaSpKENEwCrgHOB66vqNcB/83+newCowX8sPJL/XLiqdlfVdFVNT00t+kE2SdJJGuaTwAeBg1V1V1u+hUEAfC7JWVX1eDvFc7itPwRsmPf+9a12CHjDs+ofOfmhSxq3jVf97UT2+8h73jyR/T7fLXoEUFVPAI8l+a5W2gw8AOwDjt/Jsx24tbX3AZe3u4EuAI61U0W3A1uSrGkXf7e0miRpAoZ9FtDbgD9JcjrwMHAFg/C4OckO4FHg0tb3NuBiYBZ4qvWlqo4kuQa4u/W7uqqOjGQWkqQlGyoAqurjwPQJVm0+Qd8CrlxgO3uAPUsZoCRpefhJYEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnq1FABkOSRJP+W5ONJZlrtjCT7kzzUXte0epJcl2Q2yX1Jzp23ne2t/0NJti/PlCRJw1jKEcAbq+rVVTXdlq8CDlTVJuBAWwa4CNjUvnYC18MgMIBdwPnAecCu46EhSRq/UzkFtA3Y29p7gUvm1W+sgTuB1UnOAi4E9lfVkao6CuwHtp7C/iVJp2DYACjgH5Lck2Rnq62tqsdb+wlgbWuvAx6b996DrbZQXZI0AauG7PcDVXUoySuA/Uk+OX9lVVWSGsWAWsDsBHjlK185ik1Kkk5gqCOAqjrUXg8Df8XgHP7n2qkd2uvh1v0QsGHe29e32kL1Z+9rd1VNV9X01NTU0mYjSRraogGQ5MVJXnq8DWwBPgHsA47fybMduLW19wGXt7uBLgCOtVNFtwNbkqxpF3+3tJokaQKGOQW0FvirJMf7/2lV/X2Su4Gbk+wAHgUubf1vAy4GZoGngCsAqupIkmuAu1u/q6vqyMhmIklakkUDoKoeBl51gvoXgM0nqBdw5QLb2gPsWfowJUmj5ieBJalTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdWroAEhyWpJ7k/xNWz47yV1JZpN8KMnprf6itjzb1m+ct413tvqnklw46slIkoa3lCOAtwMPzlu+FnhfVX0HcBTY0eo7gKOt/r7WjyTnAJcB3wtsBX43yWmnNnxJ0skaKgCSrAfeDPxhWw7wJuCW1mUvcElrb2vLtPWbW/9twE1V9eWq+gwwC5w3iklIkpZu2COA3wF+BfhaW3458GRVPd2WDwLrWnsd8BhAW3+s9X+mfoL3SJLGbNEASPIjwOGqumcM4yHJziQzSWbm5ubGsUtJ6tIwRwCvA96S5BHgJganft4PrE6yqvVZDxxq7UPABoC2/mXAF+bXT/CeZ1TV7qqarqrpqampJU9IkjScRQOgqt5ZVeuraiODi7gfrqqfAO4A3tq6bQdube19bZm2/sNVVa1+WbtL6GxgE/Cxkc1EkrQkqxbvsqBfBW5K8i7gXuCGVr8B+GCSWeAIg9Cgqu5PcjPwAPA0cGVVffUU9i9JOgVLCoCq+gjwkdZ+mBPcxVNVXwJ+bIH3vxt491IHKUkaPT8JLEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVOLBkCSb0rysST/muT+JL/Z6mcnuSvJbJIPJTm91V/Ulmfb+o3ztvXOVv9UkguXa1KSpMUNcwTwZeBNVfUq4NXA1iQXANcC76uq7wCOAjta/x3A0VZ/X+tHknOAy4DvBbYCv5vktFFORpI0vEUDoAa+2BZf2L4KeBNwS6vvBS5p7W1tmbZ+c5K0+k1V9eWq+gwwC5w3kllIkpZsqGsASU5L8nHgMLAf+DTwZFU93bocBNa19jrgMYC2/hjw8vn1E7xHkjRmQwVAVX21ql4NrGfwW/t3L9eAkuxMMpNkZm5ubrl2I0ndW9JdQFX1JHAH8FpgdZJVbdV64FBrHwI2ALT1LwO+ML9+gvfM38fuqpququmpqamlDE+StATD3AU0lWR1a38z8MPAgwyC4K2t23bg1tbe15Zp6z9cVdXql7W7hM4GNgEfG9VEJElLs2rxLpwF7G137LwAuLmq/ibJA8BNSd4F3Avc0PrfAHwwySxwhMGdP1TV/UluBh4AngaurKqvjnY6kqRhLRoAVXUf8JoT1B/mBHfxVNWXgB9bYFvvBt699GFKkkbNTwJLUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnq1KIBkGRDkjuSPJDk/iRvb/UzkuxP8lB7XdPqSXJdktkk9yU5d962trf+DyXZvnzTkiQtZpgjgKeBd1TVOcAFwJVJzgGuAg5U1SbgQFsGuAjY1L52AtfDIDCAXcD5wHnAruOhIUkav0UDoKoer6p/ae3/Ah4E1gHbgL2t217gktbeBtxYA3cCq5OcBVwI7K+qI1V1FNgPbB3pbCRJQ1vSNYAkG4HXAHcBa6vq8bbqCWBta68DHpv3toOttlD92fvYmWQmyczc3NxShidJWoKhAyDJS4C/AH6hqv5z/rqqKqBGMaCq2l1V01U1PTU1NYpNSpJOYKgASPJCBj/8/6Sq/rKVP9dO7dBeD7f6IWDDvLevb7WF6pKkCRjmLqAANwAPVtV7563aBxy/k2c7cOu8+uXtbqALgGPtVNHtwJYka9rF3y2tJkmagFVD9Hkd8FPAvyX5eKv9GvAe4OYkO4BHgUvbutuAi4FZ4CngCoCqOpLkGuDu1u/qqjoykllIkpZs0QCoqn8EssDqzSfoX8CVC2xrD7BnKQOUJC0PPwksSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ1aNACS7ElyOMkn5tXOSLI/yUPtdU2rJ8l1SWaT3Jfk3Hnv2d76P5Rk+/JMR5I0rGGOAP4Y2Pqs2lXAgaraBBxoywAXAZva107gehgEBrALOB84D9h1PDQkSZOxaABU1UeBI88qbwP2tvZe4JJ59Rtr4E5gdZKzgAuB/VV1pKqOAvv5+lCRJI3RyV4DWFtVj7f2E8Da1l4HPDav38FWW6guSZqQU74IXFUF1AjGAkCSnUlmkszMzc2NarOSpGc52QD4XDu1Q3s93OqHgA3z+q1vtYXqX6eqdlfVdFVNT01NneTwJEmLOdkA2Accv5NnO3DrvPrl7W6gC4Bj7VTR7cCWJGvaxd8trSZJmpBVi3VI8mfAG4AzkxxkcDfPe4Cbk+wAHgUubd1vAy4GZoGngCsAqupIkmuAu1u/q6vq2ReWJUljtGgAVNWPL7Bq8wn6FnDlAtvZA+xZ0ugkScvGTwJLUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1atGngUrSpG286m8nst9H3vPmiex3XDwCkKROGQCS1CkDQJI65TUAaYWZ1PlwPf94BCBJnTIAJKlTngKSTpKnYp7/Jvk9HsctqB4BSFKnDABJ6pQBIEmdMgAkqVMGgCR1auwBkGRrkk8lmU1y1bj3L0kaGOttoElOAz4A/DBwELg7yb6qemCc49Dzh7diSidv3J8DOA+YraqHAZLcBGwDliUAfITs+PiDWFp5xh0A64DH5i0fBM4f8xiet/whLGkpnnOfBE6yE9jZFr+Y5FOnsLkzgc+f+qiWJteOe4/PmMh8J8w596G7OefaU5rztw3TadwBcAjYMG95fas9o6p2A7tHsbMkM1U1PYptrQS9zReccy+c8/IY911AdwObkpyd5HTgMmDfmMcgSWLMRwBV9XSSnwduB04D9lTV/eMcgyRpYOzXAKrqNuC2Me1uJKeSVpDe5gvOuRfOeRmkqpZ7H5Kk5yAfBSFJnVrxAbDYoyWSvCjJh9r6u5JsHP8oR2uIOf9SkgeS3JfkQJKhbgl7Lhv2ESJJfjRJJVnxd4wMM+ckl7bv9f1J/nTcYxy1If5uvzLJHUnubX+/L57EOEclyZ4kh5N8YoH1SXJd+/O4L8m5Ix1AVa3YLwYXkj8NfDtwOvCvwDnP6vNzwO+19mXAhyY97jHM+Y3At7T2z/Yw59bvpcBHgTuB6UmPewzf503AvcCatvyKSY97DHPeDfxsa58DPDLpcZ/inF8PnAt8YoH1FwN/BwS4ALhrlPtf6UcAzzxaoqq+Ahx/tMR824C9rX0LsDlJxjjGUVt0zlV1R1U91RbvZPB5i5VsmO8zwDXAtcCXxjm4ZTLMnH8G+EBVHQWoqsNjHuOoDTPnAr61tV8G/McYxzdyVfVR4Mg36LINuLEG7gRWJzlrVPtf6QFwokdLrFuoT1U9DRwDXj6W0S2PYeY83w4Gv0GsZIvOuR0ab6iq58vzMIb5Pn8n8J1J/inJnUm2jm10y2OYOf8G8JNJDjK4m/Bt4xnaxCz13/uSPOceBaHRSfKTwDTwg5Mey3JK8gLgvcBPT3go47aKwWmgNzA4yvtoku+vqicnOqrl9ePAH1fVbyd5LfDBJN9XVV+b9MBWopV+BLDooyXm90myisFh4xfGMrrlMcycSfJDwK8Db6mqL49pbMtlsTm/FPg+4CNJHmFwrnTfCr8QPMz3+SCwr6r+p6o+A/w7g0BYqYaZ8w7gZoCq+mfgmxg8J+j5aqh/7ydrpQfAMI+W2Adsb+23Ah+udnVlhVp0zkleA/w+gx/+K/28MCwy56o6VlVnVtXGqtrI4LrHW6pqZjLDHYlh/m7/NYPf/klyJoNTQg+Pc5AjNsycPwtsBkjyPQwCYG6soxyvfcDl7W6gC4BjVfX4qDa+ok8B1QKPlkhyNTBTVfuAGxgcJs4yuNhy2eRGfOqGnPNvAS8B/rxd7/5sVb1lYoM+RUPO+XllyDnfDmxJ8gDwVeCXq2rFHt0OOed3AH+Q5BcZXBD+6ZX8C12SP2MQ4me26xq7gBcCVNXvMbjOcTEwCzwFXDHS/a/gPztJ0ilY6aeAJEknyQCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlT/ws3yRyA8mIy+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# retention rate\n",
    "ret = df['retention_rate.four_year.full_time']\n",
    "plt.hist(ret);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `StandardScaler` from sklearn to convert this distribution to a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[target.columns] = scaler.fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 1: graduation rates\n",
    "grad = target['rate_suppressed.four_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 2: retention rates\n",
    "ret = target['retention_rate.four_year.full_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[features.columns] = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 decimal places for retention rates and 2 decimal places for graduation rates. To learn features and each and every value of target variable we will need a very large dataset. For this problem, we will round off target variables and features to 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in features:\n",
    "    features[col] = [round(elem, 2) for elem in features[col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = [round(elem, 2) for elem in ret]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = [round(elem, 2) for elem in grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle and split data\n",
    "\n",
    "Now we will perform train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, grad_train, grad_test = train_test_split(features,\n",
    "                                                   grad,\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, ret_train, ret_test = train_test_split(features,\n",
    "                                                        ret,\n",
    "                                                        test_size = 0.2,\n",
    "                                                        random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 6074 samples.\n",
      "Testing set has 1519 samples.\n"
     ]
    }
   ],
   "source": [
    "## show the results of the split\n",
    "\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model performance\n",
    "### Benchmark Model:\n",
    "\n",
    "This is a simple linear regression model. We will be using `r2_score` as the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "mod = LinearRegression()\n",
    "mod_grad = mod.fit(X_train, grad_train)\n",
    "mod_ret = mod.fit(X_train, ret_train)\n",
    "grad_pred = mod_grad.predict(X_test)\n",
    "ret_pred = mod_ret.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating performance of benchmark model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score for graduation rate is:  0.44002147780130296\n",
      "r2_score for retention rate is:  0.24679260562716832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(\"r2_score for graduation rate is: \", r2_score(grad_test, grad_pred))\n",
    "print(\"r2_score for retention rate is: \", r2_score(ret_test, ret_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use other regression models from sklearn to beat the performance of linear regressor for graduation and retention rates. Scores of benchmark model are:\n",
    "\n",
    "$$graduation\\ rate  = 0.44$$\n",
    "$$retention\\ rate  = 0.25$$\n",
    "\n",
    "> Tip: The models given here are in alphabetical order.\n",
    "\n",
    "### 1. AdaBoost Regressor:\n",
    "\n",
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "mod = AdaBoostRegressor(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_grad = mod.fit(X_train, grad_train)\n",
    "mod_ret = mod.fit(X_train, ret_train)\n",
    "grad_pred = mod_grad.predict(X_test)\n",
    "ret_pred = mod_ret.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score for graduation rate is:  -0.03964124575298711\n",
      "r2_score for retention rate is:  -0.0680021862380169\n"
     ]
    }
   ],
   "source": [
    "print(\"r2_score for graduation rate is: \", r2_score(grad_test, grad_pred))\n",
    "print(\"r2_score for retention rate is: \", r2_score(ret_test, ret_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree Regressor\n",
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "mod = DecisionTreeRegressor(max_depth = 3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_grad = mod.fit(X_train, grad_train)\n",
    "mod_ret = mod.fit(X_train, ret_train)\n",
    "grad_pred = mod_grad.predict(X_test)\n",
    "ret_pred = mod_ret.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score for graduation rate is:  0.37608810010731486\n",
      "r2_score for retention rate is:  0.2056060787328855\n"
     ]
    }
   ],
   "source": [
    "print(\"r2_score for graduation rate is: \", r2_score(grad_test, grad_pred))\n",
    "print(\"r2_score for retention rate is: \", r2_score(ret_test, ret_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extra Trees Regressor\n",
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "mod = ExtraTreesRegressor(max_depth=2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_grad = mod.fit(X_train, grad_train)\n",
    "mod_ret = mod.fit(X_train, ret_train)\n",
    "grad_pred = mod_grad.predict(X_test)\n",
    "ret_pred = mod_ret.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score for graduation rate is:  0.31427223375283475\n",
      "r2_score for retention rate is:  0.16577527363515066\n"
     ]
    }
   ],
   "source": [
    "print(\"r2_score for graduation rate is: \", r2_score(grad_test, grad_pred))\n",
    "print(\"r2_score for retention rate is: \", r2_score(ret_test, ret_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gradient Boosting Regressor\n",
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "mod = GradientBoostingRegressor(max_depth = 2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_grad = mod.fit(X_train, grad_train)\n",
    "mod_ret = mod.fit(X_train, ret_train)\n",
    "grad_pred = mod_grad.predict(X_test)\n",
    "ret_pred = mod_ret.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score for graduation rate is:  0.49414531023391783\n",
      "r2_score for retention rate is:  0.29773197312904154\n"
     ]
    }
   ],
   "source": [
    "print(\"r2_score for graduation rate is: \", r2_score(grad_test, grad_pred))\n",
    "print(\"r2_score for retention rate is: \", r2_score(ret_test, ret_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Light GBM\n",
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.847599\tvalid_0's l2: 0.941291\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's auc: 0.855067\tvalid_0's l2: 0.915154\n",
      "[3]\tvalid_0's auc: 0.854163\tvalid_0's l2: 0.891847\n",
      "[4]\tvalid_0's auc: 0.872946\tvalid_0's l2: 0.870882\n",
      "[5]\tvalid_0's auc: 0.88995\tvalid_0's l2: 0.849347\n",
      "[6]\tvalid_0's auc: 0.879313\tvalid_0's l2: 0.831072\n",
      "[7]\tvalid_0's auc: 0.867848\tvalid_0's l2: 0.813667\n",
      "[8]\tvalid_0's auc: 0.869469\tvalid_0's l2: 0.798278\n",
      "[9]\tvalid_0's auc: 0.865394\tvalid_0's l2: 0.784291\n",
      "[10]\tvalid_0's auc: 0.873003\tvalid_0's l2: 0.77289\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's auc: 0.88995\tvalid_0's l2: 0.849347\n",
      "[1]\tvalid_0's auc: 0.868116\tvalid_0's l2: 0.95429\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's auc: 0.853333\tvalid_0's l2: 0.896318\n",
      "[3]\tvalid_0's auc: 0.872905\tvalid_0's l2: 0.845373\n",
      "[4]\tvalid_0's auc: 0.872901\tvalid_0's l2: 0.798856\n",
      "[5]\tvalid_0's auc: 0.877582\tvalid_0's l2: 0.755479\n",
      "[6]\tvalid_0's auc: 0.87162\tvalid_0's l2: 0.717024\n",
      "[7]\tvalid_0's auc: 0.875059\tvalid_0's l2: 0.680762\n",
      "[8]\tvalid_0's auc: 0.877867\tvalid_0's l2: 0.648392\n",
      "[9]\tvalid_0's auc: 0.885633\tvalid_0's l2: 0.618209\n",
      "[10]\tvalid_0's auc: 0.886258\tvalid_0's l2: 0.592964\n",
      "[11]\tvalid_0's auc: 0.883396\tvalid_0's l2: 0.569404\n",
      "[12]\tvalid_0's auc: 0.883759\tvalid_0's l2: 0.547473\n",
      "[13]\tvalid_0's auc: 0.885972\tvalid_0's l2: 0.529865\n",
      "[14]\tvalid_0's auc: 0.887085\tvalid_0's l2: 0.511951\n",
      "[15]\tvalid_0's auc: 0.88935\tvalid_0's l2: 0.494973\n",
      "[16]\tvalid_0's auc: 0.887534\tvalid_0's l2: 0.479547\n",
      "[17]\tvalid_0's auc: 0.887876\tvalid_0's l2: 0.466516\n",
      "[18]\tvalid_0's auc: 0.88776\tvalid_0's l2: 0.453303\n",
      "[19]\tvalid_0's auc: 0.88507\tvalid_0's l2: 0.442452\n",
      "[20]\tvalid_0's auc: 0.887613\tvalid_0's l2: 0.431568\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's auc: 0.88935\tvalid_0's l2: 0.494973\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_ret_train = lgb.Dataset(X_train, ret_train)\n",
    "lgb_ret_eval = lgb.Dataset(X_test, ret_test, reference=lgb_ret_train)\n",
    "lgb_grad_train = lgb.Dataset(X_train, grad_train)\n",
    "lgb_grad_eval = lgb.Dataset(X_test, grad_test, reference=lgb_grad_train)\n",
    "\n",
    "\n",
    "\n",
    "# specify configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'auc'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "}\n",
    "\n",
    "# train\n",
    "gbm_ret = lgb.train(params,\n",
    "                lgb_ret_train,\n",
    "                num_boost_round=20,\n",
    "                valid_sets=lgb_ret_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "gbm_grad = lgb.train(params,\n",
    "                lgb_grad_train,\n",
    "                num_boost_round=20,\n",
    "                valid_sets=lgb_grad_eval,\n",
    "                early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_pred = gbm_ret.predict(X_test, num_iteration=gbm_ret.best_iteration)\n",
    "grad_pred = gbm_grad.predict(X_test, num_iteration=gbm_grad.best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score for graduation rate is:  0.512326958303285\n",
      "r2_score for retention rate is:  0.12326415805086066\n"
     ]
    }
   ],
   "source": [
    "print('r2_score for graduation rate is: ', r2_score(grad_test, grad_pred))\n",
    "print('r2_score for retention rate is: ', r2_score(ret_test, ret_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Random Forest Regressor\n",
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "mod = RandomForestRegressor(max_depth=2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_grad = mod.fit(X_train, grad_train)\n",
    "mod_ret = mod.fit(X_train, ret_train)\n",
    "grad_pred = mod_grad.predict(X_test)\n",
    "ret_pred = mod_ret.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score for graduation rate is:  0.3322280376391322\n",
      "r2_score for retention rate is:  0.17193729687048087\n"
     ]
    }
   ],
   "source": [
    "print(\"r2_score for graduation rate is: \", r2_score(grad_test, grad_pred))\n",
    "print(\"r2_score for retention rate is: \", r2_score(ret_test, ret_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing models\n",
    "\n",
    "Below is a summary of `r2_score`  for retention and graduation rates from the above models:\n",
    "\n",
    "|Model|Graduation score|Retention score|\n",
    "|------|------|------|\n",
    "|AdaBoost|-0.04|-0.07|\n",
    "|Decision Tree Regressor|0.38|0.21|\n",
    "|Extra Trees Regressor|0.31|0.17|\n",
    "|Gradient Boosting Regressor|0.49|**0.30**|\n",
    "|Light GBM|**0.57**|0.12|\n",
    "|Random Forest Regressor|0.33|0.17|\n",
    "\n",
    "Our benchmark metrics were:\n",
    "\n",
    "$$r2\\ score\\ for\\ graduation = 0.44$$\n",
    "$$r2\\ score\\ for\\ retention = 0.25$$\n",
    "\n",
    "Final model for **graduation rates** is **Light GBM** and for **retention rates** is **Gradient Boosting Regressor**.\n",
    "\n",
    "Now let see if we can improve these models:\n",
    "\n",
    "## Hyperparameter tuning\n",
    "\n",
    "### Light GBM:\n",
    "\n",
    "We got good relatively good prediction on graduation rates. I think we can improve performance on retention rates also by doing parameter tuning.\n",
    "\n",
    "Doing parameter tuning for graduation rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[1]\tvalid_0's l1: 0.463624\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's l1: 0.44885\n",
      "[3]\tvalid_0's l1: 0.435586\n",
      "[4]\tvalid_0's l1: 0.423064\n",
      "[5]\tvalid_0's l1: 0.41128\n",
      "[6]\tvalid_0's l1: 0.40056\n",
      "[7]\tvalid_0's l1: 0.389949\n",
      "[8]\tvalid_0's l1: 0.380402\n",
      "[9]\tvalid_0's l1: 0.371565\n",
      "[10]\tvalid_0's l1: 0.3634\n",
      "[11]\tvalid_0's l1: 0.355429\n",
      "[12]\tvalid_0's l1: 0.348253\n",
      "[13]\tvalid_0's l1: 0.340781\n",
      "[14]\tvalid_0's l1: 0.333629\n",
      "[15]\tvalid_0's l1: 0.327243\n",
      "[16]\tvalid_0's l1: 0.321094\n",
      "[17]\tvalid_0's l1: 0.31574\n",
      "[18]\tvalid_0's l1: 0.311088\n",
      "[19]\tvalid_0's l1: 0.306312\n",
      "[20]\tvalid_0's l1: 0.301818\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.301818\n",
      "Start predicting...\n",
      "The r2_score for graduation rate is: 0.7603448928102201\n",
      "Feature importances: [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 22, 2, 2, 2, 4, 0, 37, 0, 0, 0, 1, 2, 0, 4, 0, 6, 0, 0, 3, 0, 1, 0, 4, 0, 2, 2, 0, 0, 0, 9, 12, 1, 0, 6, 5, 0, 7, 0, 1, 0, 0, 11, 4, 14, 0, 0, 35, 9, 4, 8, 11, 4, 3, 4, 6, 3, 21, 0, 0, 20, 26, 22, 22, 15, 11, 45, 0, 98, 0, 5, 0, 6, 0, 11, 2, 2, 0, 0, 7, 20, 12, 0, 0, 0, 0]\n",
      "Best parameters found by grid search are: {'learning_rate': 0.1, 'n_estimators': 40}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# load or create your dataset\n",
    "#print('Load data...')\n",
    "##df_train = X_train\n",
    "#df_test = X_test\n",
    "\n",
    "#y_train = df_train[0].values\n",
    "#y_test = df_test[0].values\n",
    "#X_train = df_train.drop(0, axis=1).values\n",
    "#X_test = df_test.drop(0, axis=1).values\n",
    "\n",
    "#y_train = grad_train[0].values\n",
    "#y_test = grad_test[0].values\n",
    "#X_train = X_train.drop(0, axis=1).values\n",
    "#X_test = X_test.drop(0, axis=1).values\n",
    "\n",
    "lgb_grad_train = lgb.Dataset(X_train, grad_train)\n",
    "lgb_grad_eval = lgb.Dataset(X_test, grad_test, reference=lgb_grad_train)\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.LGBMRegressor(objective='regression',\n",
    "                        num_leaves=31,\n",
    "                        learning_rate=0.05,\n",
    "                        n_estimators=20)\n",
    "gbm.fit(X_train, grad_train,\n",
    "        eval_set=[(X_test, grad_test)],\n",
    "        eval_metric='l1',\n",
    "        early_stopping_rounds=5)\n",
    "\n",
    "print('Start predicting...')\n",
    "# predict\n",
    "grad_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
    "# eval\n",
    "print('The r2_score for graduation rate is:', r2_score(grad_test, grad_pred) ** 0.5)\n",
    "\n",
    "# feature importances\n",
    "print('Feature importances:', list(gbm.feature_importances_))\n",
    "\n",
    "# other scikit-learn modules\n",
    "estimator = lgb.LGBMRegressor(num_leaves=31)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [20, 40]\n",
    "}\n",
    "\n",
    "gbm = GridSearchCV(estimator, param_grid)\n",
    "\n",
    "gbm.fit(X_train, grad_train)\n",
    "\n",
    "print('Best parameters found by grid search are:', gbm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the updated parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[1]\tvalid_0's l1: 0.448439\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's l1: 0.421679\n",
      "[3]\tvalid_0's l1: 0.398159\n",
      "[4]\tvalid_0's l1: 0.378211\n",
      "[5]\tvalid_0's l1: 0.36072\n",
      "[6]\tvalid_0's l1: 0.345565\n",
      "[7]\tvalid_0's l1: 0.332063\n",
      "[8]\tvalid_0's l1: 0.319979\n",
      "[9]\tvalid_0's l1: 0.310626\n",
      "[10]\tvalid_0's l1: 0.302316\n",
      "[11]\tvalid_0's l1: 0.295053\n",
      "[12]\tvalid_0's l1: 0.290236\n",
      "[13]\tvalid_0's l1: 0.284533\n",
      "[14]\tvalid_0's l1: 0.278968\n",
      "[15]\tvalid_0's l1: 0.275034\n",
      "[16]\tvalid_0's l1: 0.270575\n",
      "[17]\tvalid_0's l1: 0.268038\n",
      "[18]\tvalid_0's l1: 0.265132\n",
      "[19]\tvalid_0's l1: 0.262349\n",
      "[20]\tvalid_0's l1: 0.260263\n",
      "[21]\tvalid_0's l1: 0.258601\n",
      "[22]\tvalid_0's l1: 0.257229\n",
      "[23]\tvalid_0's l1: 0.255428\n",
      "[24]\tvalid_0's l1: 0.25423\n",
      "[25]\tvalid_0's l1: 0.253001\n",
      "[26]\tvalid_0's l1: 0.252466\n",
      "[27]\tvalid_0's l1: 0.252726\n",
      "[28]\tvalid_0's l1: 0.252501\n",
      "[29]\tvalid_0's l1: 0.252492\n",
      "[30]\tvalid_0's l1: 0.251607\n",
      "[31]\tvalid_0's l1: 0.251237\n",
      "[32]\tvalid_0's l1: 0.251127\n",
      "[33]\tvalid_0's l1: 0.250958\n",
      "[34]\tvalid_0's l1: 0.250608\n",
      "[35]\tvalid_0's l1: 0.250298\n",
      "[36]\tvalid_0's l1: 0.250287\n",
      "[37]\tvalid_0's l1: 0.251014\n",
      "[38]\tvalid_0's l1: 0.251826\n",
      "[39]\tvalid_0's l1: 0.252397\n",
      "[40]\tvalid_0's l1: 0.252609\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[36]\tvalid_0's l1: 0.250287\n",
      "Start predicting...\n",
      "The r2_score for graduation rate is: 0.832961990243017\n",
      "Feature importances: [0, 0, 0, 0, 0, 0, 0, 0, 10, 7, 2, 17, 8, 4, 10, 7, 0, 31, 0, 0, 1, 9, 9, 3, 19, 0, 15, 3, 3, 4, 0, 11, 3, 10, 0, 1, 6, 0, 4, 3, 16, 16, 6, 0, 8, 10, 2, 8, 0, 2, 0, 0, 24, 19, 27, 2, 0, 46, 25, 16, 22, 31, 20, 7, 13, 34, 20, 59, 0, 7, 35, 27, 36, 56, 22, 14, 64, 0, 94, 0, 8, 2, 9, 0, 10, 2, 0, 0, 2, 18, 25, 13, 3, 0, 0, 0]\n",
      "Best parameters found by grid search are: {'learning_rate': 0.1, 'n_estimators': 40}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# load or create your dataset\n",
    "lgb_grad_train = lgb.Dataset(X_train, grad_train)\n",
    "lgb_grad_eval = lgb.Dataset(X_test, grad_test, reference=lgb_grad_train)\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.LGBMRegressor(objective='regression',\n",
    "                        num_leaves=31,\n",
    "                        learning_rate=0.1,\n",
    "                        n_estimators=40)\n",
    "gbm.fit(X_train, grad_train,\n",
    "        eval_set=[(X_test, grad_test)],\n",
    "        eval_metric='l1',\n",
    "        early_stopping_rounds=5)\n",
    "\n",
    "print('Start predicting...')\n",
    "# predict\n",
    "grad_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
    "# eval\n",
    "print('The r2_score for graduation rate is:', r2_score(grad_test, grad_pred) ** 0.5)\n",
    "\n",
    "# feature importances\n",
    "\n",
    "features_grad = []\n",
    "features_grad.extend(list(gbm.feature_importances_))\n",
    "print('Feature importances:', list(gbm.feature_importances_))\n",
    "\n",
    "# other scikit-learn modules\n",
    "estimator = lgb.LGBMRegressor(num_leaves=31)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [20, 40]\n",
    "}\n",
    "\n",
    "gbm = GridSearchCV(estimator, param_grid)\n",
    "\n",
    "gbm.fit(X_train, grad_train)\n",
    "\n",
    "print('Best parameters found by grid search are:', gbm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `features_grad` later for seeing the important features.\n",
    "\n",
    "Hyperparameter tuning for retention rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[1]\tvalid_0's l1: 0.444161\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's l1: 0.434988\n",
      "[3]\tvalid_0's l1: 0.426778\n",
      "[4]\tvalid_0's l1: 0.418795\n",
      "[5]\tvalid_0's l1: 0.411847\n",
      "[6]\tvalid_0's l1: 0.40431\n",
      "[7]\tvalid_0's l1: 0.397648\n",
      "[8]\tvalid_0's l1: 0.392474\n",
      "[9]\tvalid_0's l1: 0.387653\n",
      "[10]\tvalid_0's l1: 0.381955\n",
      "[11]\tvalid_0's l1: 0.377772\n",
      "[12]\tvalid_0's l1: 0.37318\n",
      "[13]\tvalid_0's l1: 0.369115\n",
      "[14]\tvalid_0's l1: 0.365393\n",
      "[15]\tvalid_0's l1: 0.361226\n",
      "[16]\tvalid_0's l1: 0.357428\n",
      "[17]\tvalid_0's l1: 0.354652\n",
      "[18]\tvalid_0's l1: 0.351916\n",
      "[19]\tvalid_0's l1: 0.348916\n",
      "[20]\tvalid_0's l1: 0.345653\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.345653\n",
      "Start predicting...\n",
      "The r2_score for retention rate is: 0.527906226014502\n",
      "Feature importances: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 18, 3, 0, 10, 6, 0, 20, 0, 0, 0, 0, 4, 1, 18, 2, 2, 0, 1, 0, 0, 10, 6, 3, 0, 1, 0, 0, 0, 0, 4, 0, 2, 0, 0, 24, 1, 4, 0, 0, 0, 0, 8, 7, 42, 0, 0, 22, 7, 18, 19, 24, 25, 8, 15, 9, 12, 19, 0, 0, 22, 28, 6, 32, 24, 6, 23, 0, 48, 0, 0, 0, 3, 0, 0, 0, 5, 0, 0, 0, 16, 9, 1, 0, 0, 0]\n",
      "Best parameters found by grid search are: {'learning_rate': 0.1, 'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# load or create your dataset\n",
    "#print('Load data...')\n",
    "##df_train = X_train\n",
    "#df_test = X_test\n",
    "\n",
    "#y_train = df_train[0].values\n",
    "#y_test = df_test[0].values\n",
    "#X_train = df_train.drop(0, axis=1).values\n",
    "#X_test = df_test.drop(0, axis=1).values\n",
    "\n",
    "#y_train = grad_train[0].values\n",
    "#y_test = grad_test[0].values\n",
    "#X_train = X_train.drop(0, axis=1).values\n",
    "#X_test = X_test.drop(0, axis=1).values\n",
    "\n",
    "lgb_ret_train = lgb.Dataset(X_train, ret_train)\n",
    "lgb_ret_eval = lgb.Dataset(X_test, ret_test, reference=lgb_ret_train)\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.LGBMRegressor(objective='regression',\n",
    "                        num_leaves=31,\n",
    "                        learning_rate=0.05,\n",
    "                        n_estimators=20)\n",
    "gbm.fit(X_train, ret_train,\n",
    "        eval_set=[(X_test, ret_test)],\n",
    "        eval_metric='l1',\n",
    "        early_stopping_rounds=5)\n",
    "\n",
    "print('Start predicting...')\n",
    "# predict\n",
    "ret_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
    "# eval\n",
    "print('The r2_score for retention rate is:', r2_score(ret_test, ret_pred) ** 0.5)\n",
    "\n",
    "# feature importances\n",
    "print('Feature importances:', list(gbm.feature_importances_))\n",
    "\n",
    "# other scikit-learn modules\n",
    "estimator = lgb.LGBMRegressor(num_leaves=31)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [20, 40]\n",
    "}\n",
    "\n",
    "gbm = GridSearchCV(estimator, param_grid)\n",
    "\n",
    "gbm.fit(X_train, ret_train)\n",
    "\n",
    "print('Best parameters found by grid search are:', gbm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the updated parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[1]\tvalid_0's l1: 0.434512\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's l1: 0.417363\n",
      "[3]\tvalid_0's l1: 0.401554\n",
      "[4]\tvalid_0's l1: 0.388436\n",
      "[5]\tvalid_0's l1: 0.378888\n",
      "[6]\tvalid_0's l1: 0.369773\n",
      "[7]\tvalid_0's l1: 0.360875\n",
      "[8]\tvalid_0's l1: 0.353283\n",
      "[9]\tvalid_0's l1: 0.347803\n",
      "[10]\tvalid_0's l1: 0.342409\n",
      "[11]\tvalid_0's l1: 0.337733\n",
      "[12]\tvalid_0's l1: 0.333652\n",
      "[13]\tvalid_0's l1: 0.330051\n",
      "[14]\tvalid_0's l1: 0.326931\n",
      "[15]\tvalid_0's l1: 0.324373\n",
      "[16]\tvalid_0's l1: 0.321548\n",
      "[17]\tvalid_0's l1: 0.31978\n",
      "[18]\tvalid_0's l1: 0.317162\n",
      "[19]\tvalid_0's l1: 0.315791\n",
      "[20]\tvalid_0's l1: 0.313905\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.313905\n",
      "Start predicting...\n",
      "The r2_score for retention rate is: 0.5764331025515608\n",
      "Feature importances: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 13, 4, 0, 8, 3, 0, 12, 0, 0, 1, 2, 2, 0, 15, 2, 2, 0, 2, 2, 2, 4, 8, 4, 0, 0, 0, 0, 0, 1, 5, 2, 0, 0, 2, 19, 3, 2, 0, 0, 0, 0, 5, 11, 34, 0, 0, 26, 8, 20, 25, 26, 25, 14, 16, 7, 13, 19, 1, 1, 24, 27, 7, 35, 26, 7, 18, 0, 45, 0, 2, 1, 5, 0, 1, 1, 4, 0, 0, 1, 16, 7, 0, 0, 0, 1]\n",
      "Best parameters found by grid search are: {'learning_rate': 0.1, 'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lgb_ret_train = lgb.Dataset(X_train, ret_train)\n",
    "lgb_ret_eval = lgb.Dataset(X_test, ret_test, reference=lgb_ret_train)\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.LGBMRegressor(objective='regression',\n",
    "                        num_leaves=31,\n",
    "                        learning_rate=0.1,\n",
    "                        n_estimators=20)\n",
    "gbm.fit(X_train, ret_train,\n",
    "        eval_set=[(X_test, ret_test)],\n",
    "        eval_metric='l1',\n",
    "        early_stopping_rounds=5)\n",
    "\n",
    "print('Start predicting...')\n",
    "# predict\n",
    "ret_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
    "# eval\n",
    "print('The r2_score for retention rate is:', r2_score(ret_test, ret_pred) ** 0.5)\n",
    "\n",
    "# feature importances\n",
    "features_ret = []\n",
    "features_ret.extend(list(gbm.feature_importances_))\n",
    "print('Feature importances:', list(gbm.feature_importances_))\n",
    "\n",
    "# other scikit-learn modules\n",
    "estimator = lgb.LGBMRegressor(num_leaves=31)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [20, 40]\n",
    "}\n",
    "\n",
    "gbm = GridSearchCV(estimator, param_grid)\n",
    "\n",
    "gbm.fit(X_train, ret_train)\n",
    "\n",
    "print('Best parameters found by grid search are:', gbm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `features_ret` later for seeing the important features.\n",
    "Final `r2_score` of LightGBM model for:\n",
    "$$graduation\\ rate = 0.83$$\n",
    "$$retention\\ rate = 0.58$$\n",
    "\n",
    "These results are above the benchmark value so it is ok to accept this as our final model. But let us check if Gradient Boosting Regressor model can give even better results.\n",
    "\n",
    "### Gradient Boosting Regressor\n",
    "\n",
    "We can see that hyperparameter tuning in LightGBM has increased prediction on retention rate and it is now even higher than this model. Let us see if we can improve this model to beat LightGBM performance.\n",
    "\n",
    "`r2_score` of untuned Gradient Boosting Regressor for:\n",
    "$$graduation\\ rate = 0.31$$\n",
    "$$retention\\ rate = 0.17$$\n",
    "\n",
    "`r2_score` for both labels are less than the improved LightGBM model. Let us see if we can improve this model by performing hyperparameter tuning.\n",
    "\n",
    "For graduation rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score for graduation rate: 0.6922465070848249\n",
      "Best Estimator learned through GridSearch\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=6, max_features=1.0,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=3,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "gbrt=GradientBoostingRegressor(n_estimators=100)\n",
    "gbrt.fit(X_train, grad_train)\n",
    "grad_pred=gbrt.predict(X_test)\n",
    "\n",
    "#print(\"Feature Importances\")\n",
    "#print(gbrt.feature_importances_)\n",
    "\n",
    "print(\"r2_score for graduation rate: {}\" .format(r2_score(grad_test, grad_pred)))\n",
    "\n",
    "def GradientBooster(param_grid, n_jobs):\n",
    "    estimator = GradientBoostingRegressor()\n",
    "    #Choose cross-validation generator - let's choose ShuffleSplit which randomly shuffles and selects Train and CV sets #for each iteration. There are other methods like the KFold split.\n",
    "    cv = ShuffleSplit(X_train.shape[0], n_iter=10, test_size=0.2)\n",
    "    #Apply the cross-validation iterator on the Training set using GridSearchCV. This will run the classifier on the #different train/cv splits using parameters specified and return the model that has the best results #Note that we are tuning based on the F1 score 2PR/P+R where P is Precision and R is Recall. This may not always be #the best score to tune our model on. I will explore this area further in a seperate exercise. For now, we'll use F1. \n",
    "    classifier = GridSearchCV(estimator=estimator, cv=cv, param_grid=param_grid, n_jobs=n_jobs) #Also note that we're feeding multiple neighbors to the GridSearch to try out. #We'll now fit the training dataset to this classifier \n",
    "    classifier.fit(X_train, grad_train) #Let's look at the best estimator that was found by GridSearchCV \n",
    "    print(\"Best Estimator learned through GridSearch\")\n",
    "    print(classifier.best_estimator_ )\n",
    "    return cv, classifier.best_estimator_ \n",
    "\n",
    "param_grid={'n_estimators':[100],\n",
    "            'learning_rate': [0.1],# 0.05, 0.02, 0.01],\n",
    "            'max_depth':[6],#4,6],\n",
    "            'min_samples_leaf':[3],#,5,9,17],\n",
    "            'max_features':[1.0],#,0.3]#,0.1]\n",
    "           } \n",
    "n_jobs=4 \n",
    "\n",
    "#Let's fit GBRT to the digits training dataset by calling the function we just created. \n",
    "cv,best_est=GradientBooster(param_grid, n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For retention rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score for retention rate: 0.3111514933522639\n",
      "Best Estimator learned through GridSearch\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=6, max_features=1.0,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=3,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "gbrt=GradientBoostingRegressor(n_estimators=100)\n",
    "gbrt.fit(X_train, ret_train)\n",
    "ret_pred=gbrt.predict(X_test)\n",
    "\n",
    "#print(\"Feature Importances\")\n",
    "#print(gbrt.feature_importances_)\n",
    "\n",
    "print(\"r2_score for retention rate: {}\" .format(r2_score(ret_test, ret_pred)))\n",
    "\n",
    "def GradientBooster(param_grid, n_jobs):\n",
    "    estimator = GradientBoostingRegressor()\n",
    "    #Choose cross-validation generator - let's choose ShuffleSplit which randomly shuffles and selects Train and CV sets #for each iteration. There are other methods like the KFold split.\n",
    "    cv = ShuffleSplit(X_train.shape[0], n_iter=10, test_size=0.2)\n",
    "    #Apply the cross-validation iterator on the Training set using GridSearchCV. This will run the classifier on the #different train/cv splits using parameters specified and return the model that has the best results #Note that we are tuning based on the F1 score 2PR/P+R where P is Precision and R is Recall. This may not always be #the best score to tune our model on. I will explore this area further in a seperate exercise. For now, we'll use F1. \n",
    "    classifier = GridSearchCV(estimator=estimator, cv=cv, param_grid=param_grid, n_jobs=n_jobs) #Also note that we're feeding multiple neighbors to the GridSearch to try out. #We'll now fit the training dataset to this classifier \n",
    "    classifier.fit(X_train, ret_train) #Let's look at the best estimator that was found by GridSearchCV \n",
    "    print(\"Best Estimator learned through GridSearch\")\n",
    "    print(classifier.best_estimator_ )\n",
    "    return cv, classifier.best_estimator_ \n",
    "\n",
    "param_grid={'n_estimators':[100],\n",
    "            'learning_rate': [0.1],# 0.05, 0.02, 0.01],\n",
    "            'max_depth':[6],#4,6],\n",
    "            'min_samples_leaf':[3],#,5,9,17],\n",
    "            'max_features':[1.0],#,0.3]#,0.1]\n",
    "           } \n",
    "n_jobs=4 \n",
    "\n",
    "#Let's fit GBRT to the digits training dataset by calling the function we just created. \n",
    "cv,best_est=GradientBooster(param_grid, n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters are same for predicting both labels. Applying the improved model by using the latest parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score for graduation rate is:  0.46394762552434343\n",
      "r2_score for retention rate is:  0.3397605056903774\n"
     ]
    }
   ],
   "source": [
    "mod = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
    "             learning_rate=0.1, loss='ls', max_depth=6, max_features=1.0,\n",
    "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "             min_impurity_split=None, min_samples_leaf=3,\n",
    "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "             n_estimators=100, presort='auto', random_state=None,\n",
    "             subsample=1.0, verbose=0, warm_start=False)\n",
    "\n",
    "# training\n",
    "\n",
    "mod_grad = mod.fit(X_train, grad_train)\n",
    "mod_ret = mod.fit(X_train, ret_train)\n",
    "\n",
    "# predicting\n",
    "\n",
    "grad_pred = mod_grad.predict(X_test)\n",
    "ret_pred = mod_ret.predict(X_test)\n",
    "\n",
    "# evaluating\n",
    "print(\"r2_score for graduation rate is: \", r2_score(grad_test, grad_pred))\n",
    "print(\"r2_score for retention rate is: \", r2_score(ret_test, ret_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramter tuning decreased `r2_score` for graduation score for graduation rates but increased for retention rates:\n",
    "Final `r2_score` of Gradient Boosting Regressor for:\n",
    "$$graduation\\ rate = 0.69$$\n",
    "$$retention\\ rate = 0.34$$\n",
    "\n",
    "LightGBM has both metrics higher than this model. So, we consider **LightGBM** as our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of final model\n",
    "\n",
    "Now let us see the features importance of final model to see which factors affect the graduation and retention rates.\n",
    "\n",
    "### Graduation rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 78 (94.000000)\n",
      "2. feature 76 (64.000000)\n",
      "3. feature 67 (59.000000)\n",
      "4. feature 73 (56.000000)\n",
      "5. feature 57 (46.000000)\n",
      "6. feature 72 (36.000000)\n",
      "7. feature 70 (35.000000)\n",
      "8. feature 65 (34.000000)\n",
      "9. feature 17 (31.000000)\n",
      "10. feature 61 (31.000000)\n",
      "11. feature 71 (27.000000)\n",
      "12. feature 54 (27.000000)\n",
      "13. feature 90 (25.000000)\n",
      "14. feature 58 (25.000000)\n",
      "15. feature 52 (24.000000)\n",
      "16. feature 60 (22.000000)\n",
      "17. feature 74 (22.000000)\n",
      "18. feature 62 (20.000000)\n",
      "19. feature 66 (20.000000)\n",
      "20. feature 53 (19.000000)\n",
      "21. feature 24 (19.000000)\n",
      "22. feature 89 (18.000000)\n",
      "23. feature 11 (17.000000)\n",
      "24. feature 41 (16.000000)\n",
      "25. feature 40 (16.000000)\n",
      "26. feature 59 (16.000000)\n",
      "27. feature 26 (15.000000)\n",
      "28. feature 75 (14.000000)\n",
      "29. feature 64 (13.000000)\n",
      "30. feature 91 (13.000000)\n",
      "31. feature 31 (11.000000)\n",
      "32. feature 84 (10.000000)\n",
      "33. feature 45 (10.000000)\n",
      "34. feature 33 (10.000000)\n",
      "35. feature 14 (10.000000)\n",
      "36. feature 8 (10.000000)\n",
      "37. feature 82 (9.000000)\n",
      "38. feature 22 (9.000000)\n",
      "39. feature 21 (9.000000)\n",
      "40. feature 12 (8.000000)\n",
      "41. feature 44 (8.000000)\n",
      "42. feature 47 (8.000000)\n",
      "43. feature 80 (8.000000)\n",
      "44. feature 9 (7.000000)\n",
      "45. feature 63 (7.000000)\n",
      "46. feature 69 (7.000000)\n",
      "47. feature 15 (7.000000)\n",
      "48. feature 36 (6.000000)\n",
      "49. feature 42 (6.000000)\n",
      "50. feature 38 (4.000000)\n",
      "51. feature 29 (4.000000)\n",
      "52. feature 13 (4.000000)\n",
      "53. feature 39 (3.000000)\n",
      "54. feature 28 (3.000000)\n",
      "55. feature 27 (3.000000)\n",
      "56. feature 23 (3.000000)\n",
      "57. feature 92 (3.000000)\n",
      "58. feature 32 (3.000000)\n",
      "59. feature 88 (2.000000)\n",
      "60. feature 85 (2.000000)\n",
      "61. feature 55 (2.000000)\n",
      "62. feature 81 (2.000000)\n",
      "63. feature 46 (2.000000)\n",
      "64. feature 49 (2.000000)\n",
      "65. feature 10 (2.000000)\n",
      "66. feature 35 (1.000000)\n",
      "67. feature 20 (1.000000)\n",
      "68. feature 4 (0.000000)\n",
      "69. feature 18 (0.000000)\n",
      "70. feature 16 (0.000000)\n",
      "71. feature 5 (0.000000)\n",
      "72. feature 6 (0.000000)\n",
      "73. feature 3 (0.000000)\n",
      "74. feature 2 (0.000000)\n",
      "75. feature 7 (0.000000)\n",
      "76. feature 1 (0.000000)\n",
      "77. feature 95 (0.000000)\n",
      "78. feature 19 (0.000000)\n",
      "79. feature 56 (0.000000)\n",
      "80. feature 93 (0.000000)\n",
      "81. feature 87 (0.000000)\n",
      "82. feature 86 (0.000000)\n",
      "83. feature 83 (0.000000)\n",
      "84. feature 79 (0.000000)\n",
      "85. feature 77 (0.000000)\n",
      "86. feature 68 (0.000000)\n",
      "87. feature 51 (0.000000)\n",
      "88. feature 25 (0.000000)\n",
      "89. feature 50 (0.000000)\n",
      "90. feature 48 (0.000000)\n",
      "91. feature 94 (0.000000)\n",
      "92. feature 43 (0.000000)\n",
      "93. feature 37 (0.000000)\n",
      "94. feature 34 (0.000000)\n",
      "95. feature 30 (0.000000)\n",
      "96. feature 0 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "indices = np.argsort(features_grad)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], features_grad[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us take some top features obtained from the importances list, i.e., indices 78, 76, 67, 73, 57, 72, 70, 65, 17, 61. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['3_yr_repayment_suppressed.overall',\n",
       "       'median_debt_suppressed.completers.overall', 'part_time_share',\n",
       "       'share_25_older', 'size', 'federal_loan_rate', 'avg_net_price.private',\n",
       "       'demographics.race_ethnicity.non_resident_alien',\n",
       "       'sat_scores.average.overall', 'demographics.race_ethnicity.asian'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns[[78, 76, 67, 73, 57, 72, 70, 65, 17, 61]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retention rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 78 (45.000000)\n",
      "2. feature 73 (35.000000)\n",
      "3. feature 54 (34.000000)\n",
      "4. feature 71 (27.000000)\n",
      "5. feature 57 (26.000000)\n",
      "6. feature 74 (26.000000)\n",
      "7. feature 61 (26.000000)\n",
      "8. feature 60 (25.000000)\n",
      "9. feature 62 (25.000000)\n",
      "10. feature 70 (24.000000)\n",
      "11. feature 59 (20.000000)\n",
      "12. feature 67 (19.000000)\n",
      "13. feature 45 (19.000000)\n",
      "14. feature 76 (18.000000)\n",
      "15. feature 64 (16.000000)\n",
      "16. feature 90 (16.000000)\n",
      "17. feature 24 (15.000000)\n",
      "18. feature 63 (14.000000)\n",
      "19. feature 66 (13.000000)\n",
      "20. feature 11 (13.000000)\n",
      "21. feature 17 (12.000000)\n",
      "22. feature 53 (11.000000)\n",
      "23. feature 14 (8.000000)\n",
      "24. feature 32 (8.000000)\n",
      "25. feature 58 (8.000000)\n",
      "26. feature 65 (7.000000)\n",
      "27. feature 75 (7.000000)\n",
      "28. feature 72 (7.000000)\n",
      "29. feature 91 (7.000000)\n",
      "30. feature 52 (5.000000)\n",
      "31. feature 82 (5.000000)\n",
      "32. feature 40 (5.000000)\n",
      "33. feature 31 (4.000000)\n",
      "34. feature 33 (4.000000)\n",
      "35. feature 86 (4.000000)\n",
      "36. feature 12 (4.000000)\n",
      "37. feature 46 (3.000000)\n",
      "38. feature 15 (3.000000)\n",
      "39. feature 30 (2.000000)\n",
      "40. feature 47 (2.000000)\n",
      "41. feature 26 (2.000000)\n",
      "42. feature 29 (2.000000)\n",
      "43. feature 28 (2.000000)\n",
      "44. feature 41 (2.000000)\n",
      "45. feature 25 (2.000000)\n",
      "46. feature 80 (2.000000)\n",
      "47. feature 22 (2.000000)\n",
      "48. feature 21 (2.000000)\n",
      "49. feature 44 (2.000000)\n",
      "50. feature 39 (1.000000)\n",
      "51. feature 20 (1.000000)\n",
      "52. feature 10 (1.000000)\n",
      "53. feature 95 (1.000000)\n",
      "54. feature 84 (1.000000)\n",
      "55. feature 85 (1.000000)\n",
      "56. feature 81 (1.000000)\n",
      "57. feature 69 (1.000000)\n",
      "58. feature 68 (1.000000)\n",
      "59. feature 89 (1.000000)\n",
      "60. feature 92 (0.000000)\n",
      "61. feature 16 (0.000000)\n",
      "62. feature 87 (0.000000)\n",
      "63. feature 88 (0.000000)\n",
      "64. feature 13 (0.000000)\n",
      "65. feature 42 (0.000000)\n",
      "66. feature 18 (0.000000)\n",
      "67. feature 9 (0.000000)\n",
      "68. feature 8 (0.000000)\n",
      "69. feature 7 (0.000000)\n",
      "70. feature 6 (0.000000)\n",
      "71. feature 5 (0.000000)\n",
      "72. feature 4 (0.000000)\n",
      "73. feature 3 (0.000000)\n",
      "74. feature 2 (0.000000)\n",
      "75. feature 1 (0.000000)\n",
      "76. feature 93 (0.000000)\n",
      "77. feature 83 (0.000000)\n",
      "78. feature 19 (0.000000)\n",
      "79. feature 49 (0.000000)\n",
      "80. feature 94 (0.000000)\n",
      "81. feature 48 (0.000000)\n",
      "82. feature 38 (0.000000)\n",
      "83. feature 37 (0.000000)\n",
      "84. feature 36 (0.000000)\n",
      "85. feature 35 (0.000000)\n",
      "86. feature 34 (0.000000)\n",
      "87. feature 50 (0.000000)\n",
      "88. feature 43 (0.000000)\n",
      "89. feature 51 (0.000000)\n",
      "90. feature 55 (0.000000)\n",
      "91. feature 56 (0.000000)\n",
      "92. feature 27 (0.000000)\n",
      "93. feature 77 (0.000000)\n",
      "94. feature 79 (0.000000)\n",
      "95. feature 23 (0.000000)\n",
      "96. feature 0 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "indices = np.argsort(features_ret)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], features_ret[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us take some top features obtained from the importances list, i.e., indices 78, 73, 54, 71, 57, 74, 61, 60, 62, 70. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['3_yr_repayment_suppressed.overall', 'share_25_older',\n",
       "       'program_percentage.business_marketing', 'pell_grant_rate', 'size',\n",
       "       '10_years_after_entry.median', 'demographics.race_ethnicity.asian',\n",
       "       'demographics.race_ethnicity.hispanic',\n",
       "       'demographics.race_ethnicity.aian', 'avg_net_price.private'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns[[78, 73, 54, 71, 57, 74, 61, 60, 62, 70]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 78 (45.000000)\n",
      "2. feature 73 (35.000000)\n",
      "3. feature 54 (34.000000)\n",
      "4. feature 71 (27.000000)\n",
      "5. feature 57 (26.000000)\n",
      "6. feature 74 (26.000000)\n",
      "7. feature 61 (26.000000)\n",
      "8. feature 60 (25.000000)\n",
      "9. feature 62 (25.000000)\n",
      "10. feature 70 (24.000000)\n",
      "11. feature 59 (20.000000)\n",
      "12. feature 67 (19.000000)\n",
      "13. feature 45 (19.000000)\n",
      "14. feature 76 (18.000000)\n",
      "15. feature 64 (16.000000)\n",
      "16. feature 90 (16.000000)\n",
      "17. feature 24 (15.000000)\n",
      "18. feature 63 (14.000000)\n",
      "19. feature 66 (13.000000)\n",
      "20. feature 11 (13.000000)\n",
      "21. feature 17 (12.000000)\n",
      "22. feature 53 (11.000000)\n",
      "23. feature 14 (8.000000)\n",
      "24. feature 32 (8.000000)\n",
      "25. feature 58 (8.000000)\n",
      "26. feature 65 (7.000000)\n",
      "27. feature 75 (7.000000)\n",
      "28. feature 72 (7.000000)\n",
      "29. feature 91 (7.000000)\n",
      "30. feature 52 (5.000000)\n",
      "31. feature 82 (5.000000)\n",
      "32. feature 40 (5.000000)\n",
      "33. feature 31 (4.000000)\n",
      "34. feature 33 (4.000000)\n",
      "35. feature 86 (4.000000)\n",
      "36. feature 12 (4.000000)\n",
      "37. feature 46 (3.000000)\n",
      "38. feature 15 (3.000000)\n",
      "39. feature 30 (2.000000)\n",
      "40. feature 47 (2.000000)\n",
      "41. feature 26 (2.000000)\n",
      "42. feature 29 (2.000000)\n",
      "43. feature 28 (2.000000)\n",
      "44. feature 41 (2.000000)\n",
      "45. feature 25 (2.000000)\n",
      "46. feature 80 (2.000000)\n",
      "47. feature 22 (2.000000)\n",
      "48. feature 21 (2.000000)\n",
      "49. feature 44 (2.000000)\n",
      "50. feature 39 (1.000000)\n",
      "51. feature 20 (1.000000)\n",
      "52. feature 10 (1.000000)\n",
      "53. feature 95 (1.000000)\n",
      "54. feature 84 (1.000000)\n",
      "55. feature 85 (1.000000)\n",
      "56. feature 81 (1.000000)\n",
      "57. feature 69 (1.000000)\n",
      "58. feature 68 (1.000000)\n",
      "59. feature 89 (1.000000)\n",
      "60. feature 92 (0.000000)\n",
      "61. feature 16 (0.000000)\n",
      "62. feature 87 (0.000000)\n",
      "63. feature 88 (0.000000)\n",
      "64. feature 13 (0.000000)\n",
      "65. feature 42 (0.000000)\n",
      "66. feature 18 (0.000000)\n",
      "67. feature 9 (0.000000)\n",
      "68. feature 8 (0.000000)\n",
      "69. feature 7 (0.000000)\n",
      "70. feature 6 (0.000000)\n",
      "71. feature 5 (0.000000)\n",
      "72. feature 4 (0.000000)\n",
      "73. feature 3 (0.000000)\n",
      "74. feature 2 (0.000000)\n",
      "75. feature 1 (0.000000)\n",
      "76. feature 93 (0.000000)\n",
      "77. feature 83 (0.000000)\n",
      "78. feature 19 (0.000000)\n",
      "79. feature 49 (0.000000)\n",
      "80. feature 94 (0.000000)\n",
      "81. feature 48 (0.000000)\n",
      "82. feature 38 (0.000000)\n",
      "83. feature 37 (0.000000)\n",
      "84. feature 36 (0.000000)\n",
      "85. feature 35 (0.000000)\n",
      "86. feature 34 (0.000000)\n",
      "87. feature 50 (0.000000)\n",
      "88. feature 43 (0.000000)\n",
      "89. feature 51 (0.000000)\n",
      "90. feature 55 (0.000000)\n",
      "91. feature 56 (0.000000)\n",
      "92. feature 27 (0.000000)\n",
      "93. feature 77 (0.000000)\n",
      "94. feature 79 (0.000000)\n",
      "95. feature 23 (0.000000)\n",
      "96. feature 0 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "# abr = AdaBoostRegressor(random_state=0)\n",
    "# abr.fit(X_train, grad_train)\n",
    "#importances = gbm.feature_importances_\n",
    "indices = np.argsort(top_features)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], top_features[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us study the common features affecting them. These are:\n",
    "\n",
    "1. `3_yr_repayment_suppressed.overall`\n",
    "\n",
    "Repayment rate depicts the fraction of borrowers at an institution who are not in default on their federal loans and who are making progress in paying them down. The rates are available for 1, 3, 5, and 7 years after entering repayment.\n",
    "\n",
    "Students who are not able to pay loans do not continue in next fiscal year. This reflects the economic condition of student and there ability to conitnue study.\n",
    "\n",
    "2. `share_25_older`\n",
    "\n",
    "This is the percentage of undergraduates older than 25. Generally undergraduate are aged between 18 and 21 years. If they are above 25, then they may have have backlogs in many courses and they are not able to clear it. As a result, they decide to drop college.\n",
    "\n",
    "3. `size`\n",
    "\n",
    "This shows the enrollment of number of undergraduate certificate/degree seeking students. It is obvious that number of student dropping college is proportional to the number of student in it.\n",
    "\n",
    "\n",
    "4. `demographics.race_ethnicity.asian`\n",
    "\n",
    "This is total share of enrollment of undergraduate degree-seeking students who are Asian. This can be because Asian people are in a foreign country and it difficult for them to adapt to the changes.\n",
    "\n",
    "5. `avg_net_price.private`\n",
    "\n",
    "This is the average net price for Title IV institutions. Title IV is a term that refers to federal financial aid funds. Federal regulations state that any federal funds disbursed to a student's account in excess of allowable charges must be delivered to the student (or parent in case of an undergraduate PLUS loan).\n",
    "\n",
    "This is similar to the first feature. Unable to pay loan results in leaving the college."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
